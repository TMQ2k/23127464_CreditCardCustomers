{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f2742079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt (KH√îNG D√ôNG csv hay scipy)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "640aea36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ define statistical helper functions!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# HELPER FUNCTIONS: Manual Statistical Tests (KH√îNG D√ôNG scipy)\n",
    "# ============================================================================\n",
    "\n",
    "def chi_square_test(observed):\n",
    "    \"\"\"\n",
    "    Chi-square test of independence (manual implementation)\n",
    "    \n",
    "    Parameters:\n",
    "    - observed: 2D array (contingency table)\n",
    "    \n",
    "    Returns:\n",
    "    - chi2: Chi-square statistic\n",
    "    - p_value: P-value\n",
    "    - dof: Degrees of freedom\n",
    "    - expected: Expected frequencies\n",
    "    \"\"\"\n",
    "    observed = np.array(observed, dtype=float)\n",
    "    \n",
    "    # Calculate row and column totals\n",
    "    row_totals = observed.sum(axis=1)\n",
    "    col_totals = observed.sum(axis=0)\n",
    "    n_total = observed.sum()\n",
    "    \n",
    "    # Calculate expected frequencies\n",
    "    expected = np.outer(row_totals, col_totals) / n_total\n",
    "    \n",
    "    # Calculate chi-square statistic\n",
    "    chi2 = np.sum((observed - expected)**2 / expected)\n",
    "    \n",
    "    # Degrees of freedom\n",
    "    dof = (observed.shape[0] - 1) * (observed.shape[1] - 1)\n",
    "    \n",
    "    # Calculate p-value using chi-square CDF (manual approximation)\n",
    "    # Using Wilson-Hilferty transformation for chi-square to normal\n",
    "    if dof > 0:\n",
    "        x = chi2 / dof\n",
    "        z = (x**(1/3) - (1 - 2/(9*dof))) / np.sqrt(2/(9*dof))\n",
    "        # Standard normal CDF approximation\n",
    "        p_value = 0.5 * (1 + np.tanh(z / np.sqrt(2)))\n",
    "        p_value = 1 - p_value  # Right tail\n",
    "    else:\n",
    "        p_value = 1.0\n",
    "    \n",
    "    return chi2, p_value, dof, expected\n",
    "\n",
    "\n",
    "def t_test_independent(sample1, sample2):\n",
    "    \"\"\"\n",
    "    Independent two-sample t-test (manual implementation)\n",
    "    \n",
    "    Parameters:\n",
    "    - sample1, sample2: 1D arrays\n",
    "    \n",
    "    Returns:\n",
    "    - t_stat: T-statistic\n",
    "    - p_value: Two-tailed p-value\n",
    "    \"\"\"\n",
    "    n1, n2 = len(sample1), len(sample2)\n",
    "    mean1, mean2 = np.mean(sample1), np.mean(sample2)\n",
    "    var1, var2 = np.var(sample1, ddof=1), np.var(sample2, ddof=1)\n",
    "    \n",
    "    # Pooled standard error\n",
    "    pooled_se = np.sqrt(var1/n1 + var2/n2)\n",
    "    \n",
    "    # T-statistic\n",
    "    t_stat = (mean1 - mean2) / pooled_se\n",
    "    \n",
    "    # Degrees of freedom (Welch-Satterthwaite)\n",
    "    dof = (var1/n1 + var2/n2)**2 / ((var1/n1)**2/(n1-1) + (var2/n2)**2/(n2-1))\n",
    "    \n",
    "    # P-value approximation using normal distribution (for large samples)\n",
    "    # For large dof, t-distribution ‚âà normal distribution\n",
    "    z = abs(t_stat)\n",
    "    p_value = 2 * (1 - 0.5 * (1 + np.tanh(z * np.sqrt(2/np.pi))))\n",
    "    \n",
    "    return t_stat, p_value\n",
    "\n",
    "print(\"‚úÖ ƒê√£ define statistical helper functions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "947f4bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ load d·ªØ li·ªáu th√†nh c√¥ng!\n",
      "üìä S·ªë d√≤ng: 10,127\n",
      "üìä S·ªë c·ªôt: 23\n",
      "\n",
      "üìã T√™n c√°c c·ªôt:\n",
      "   0. CLIENTNUM\n",
      "   1. Attrition_Flag\n",
      "   2. Customer_Age\n",
      "   3. Gender\n",
      "   4. Dependent_count\n",
      "   5. Education_Level\n",
      "   6. Marital_Status\n",
      "   7. Income_Category\n",
      "   8. Card_Category\n",
      "   9. Months_on_book\n",
      "  10. Total_Relationship_Count\n",
      "  11. Months_Inactive_12_mon\n",
      "  12. Contacts_Count_12_mon\n",
      "  13. Credit_Limit\n",
      "  14. Total_Revolving_Bal\n",
      "  15. Avg_Open_To_Buy\n",
      "  16. Total_Amt_Chng_Q4_Q1\n",
      "  17. Total_Trans_Amt\n",
      "  18. Total_Trans_Ct\n",
      "  19. Total_Ct_Chng_Q4_Q1\n",
      "  20. Avg_Utilization_Ratio\n",
      "  21. Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1\n",
      "  22. Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# B∆Ø·ªöC 1: LOAD D·ªÆ LI·ªÜU (KH√îNG D√ôNG th∆∞ vi·ªán csv)\n",
    "# ============================================================================\n",
    "# ƒê·ªçc file CSV b·∫±ng file I/O thu·∫ßn t√∫y\n",
    "file_path = '../data/raw/BankChurners.csv'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    # Lo·∫°i b·ªè d·∫•u ngo·∫∑c k√©p v√† spaces t·ª´ header\n",
    "    headers = [col.strip().strip('\"') for col in lines[0].strip().split(',')]\n",
    "    # Lo·∫°i b·ªè d·∫•u ngo·∫∑c k√©p t·ª´ data\n",
    "    rows = [[cell.strip().strip('\"') for cell in line.strip().split(',')] for line in lines[1:]]\n",
    "\n",
    "print(\"‚úÖ ƒê√£ load d·ªØ li·ªáu th√†nh c√¥ng!\")\n",
    "print(f\"üìä S·ªë d√≤ng: {len(rows):,}\")\n",
    "print(f\"üìä S·ªë c·ªôt: {len(headers)}\")\n",
    "print(f\"\\nüìã T√™n c√°c c·ªôt:\")\n",
    "for i, col in enumerate(headers):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d4624c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ ƒê√£ t√°ch d·ªØ li·ªáu th√†nh c√¥ng!\n",
      "üìä Target shape: (10127,)\n",
      "üìä Categorical data shape: (10127, 5)\n",
      "üìä Numerical data shape: (10127, 14)\n",
      "\n",
      "üè∑Ô∏è Categorical columns (5):\n",
      "  0. Gender\n",
      "  1. Education_Level\n",
      "  2. Marital_Status\n",
      "  3. Income_Category\n",
      "  4. Card_Category\n",
      "\n",
      "üî¢ Numerical columns (14):\n",
      "  0. Customer_Age\n",
      "  1. Dependent_count\n",
      "  2. Months_on_book\n",
      "  3. Total_Relationship_Count\n",
      "  4. Months_Inactive_12_mon\n",
      "  5. Contacts_Count_12_mon\n",
      "  6. Credit_Limit\n",
      "  7. Total_Revolving_Bal\n",
      "  8. Avg_Open_To_Buy\n",
      "  9. Total_Amt_Chng_Q4_Q1\n",
      "  10. Total_Trans_Amt\n",
      "  11. Total_Trans_Ct\n",
      "  12. Total_Ct_Chng_Q4_Q1\n",
      "  13. Avg_Utilization_Ratio\n"
     ]
    }
   ],
   "source": [
    "# T√°ch data th√†nh numpy arrays theo data type\n",
    "# D·ª±a tr√™n EDA, ta bi·∫øt:\n",
    "# - C·ªôt 0: CLIENTNUM (ID - s·∫Ω b·ªè)\n",
    "# - C·ªôt 1: Attrition_Flag (TARGET)\n",
    "# - C·ªôt 2-7: Categorical features\n",
    "# - C·ªôt 8-20: Numerical features\n",
    "# - C·ªôt 21-22: Naive_Bayes columns (s·∫Ω b·ªè)\n",
    "\n",
    "# ƒê·ªãnh nghƒ©a indices\n",
    "target_idx = 1\n",
    "categorical_indices = [3, 5, 6, 7, 8]  # Gender, Education, Marital, Income, Card_Category\n",
    "numerical_indices = [2, 4, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]  # Customer_Age, Dependent_count treated as numerical\n",
    "drop_indices = [0, 21, 22]  # CLIENTNUM, Naive_Bayes columns\n",
    "\n",
    "# Extract target\n",
    "target_raw = [row[target_idx] for row in rows]\n",
    "target = np.array([1 if t == 'Attrited Customer' else 0 for t in target_raw])\n",
    "\n",
    "# Extract categorical features\n",
    "categorical_data = []\n",
    "for row in rows:\n",
    "    categorical_data.append([row[i] for i in categorical_indices])\n",
    "categorical_data = np.array(categorical_data)\n",
    "\n",
    "# Extract numerical features\n",
    "numerical_data = []\n",
    "for row in rows:\n",
    "    num_row = []\n",
    "    for i in numerical_indices:\n",
    "        try:\n",
    "            num_row.append(float(row[i]))\n",
    "        except:\n",
    "            num_row.append(np.nan)  # Handle missing/invalid values\n",
    "    numerical_data.append(num_row)\n",
    "numerical_data = np.array(numerical_data)\n",
    "\n",
    "# L·∫•y t√™n c·ªôt\n",
    "categorical_cols = [headers[i] for i in categorical_indices]\n",
    "numerical_cols = [headers[i] for i in numerical_indices]\n",
    "\n",
    "print(f\"\\n‚úÖ ƒê√£ t√°ch d·ªØ li·ªáu th√†nh c√¥ng!\")\n",
    "print(f\"üìä Target shape: {target.shape}\")\n",
    "print(f\"üìä Categorical data shape: {categorical_data.shape}\")\n",
    "print(f\"üìä Numerical data shape: {numerical_data.shape}\")\n",
    "print(f\"\\nüè∑Ô∏è Categorical columns ({len(categorical_cols)}):\")\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    print(f\"  {i}. {col}\")\n",
    "print(f\"\\nüî¢ Numerical columns ({len(numerical_cols)}):\")\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    print(f\"  {i}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1642c189",
   "metadata": {},
   "source": [
    "---\n",
    "## üîç B∆Ø·ªöC 2: KI·ªÇM TRA T√çNH H·ª¢P L·ªÜ C·ª¶A GI√Å TR·ªä (DATA VALIDATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "012612ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "KI·ªÇM TRA T√çNH H·ª¢P L·ªÜ C·ª¶A C√ÅC C·ªòT NUMERICAL\n",
      "================================================================================\n",
      "\n",
      "üìä Ph√°t hi·ªán range t·ª± ƒë·ªông t·ª´ dataset:\n",
      "--------------------------------------------------------------------------------\n",
      "Customer_Age                   Range: [26.00, 73.00]\n",
      "Dependent_count                Range: [0.00, 5.00]\n",
      "Months_on_book                 Range: [13.00, 56.00]\n",
      "Total_Relationship_Count       Range: [1.00, 6.00]\n",
      "Months_Inactive_12_mon         Range: [0.00, 6.00]\n",
      "Contacts_Count_12_mon          Range: [0.00, 6.00]\n",
      "Credit_Limit                   Range: [1438.30, 34516.00]\n",
      "Total_Revolving_Bal            Range: [0.00, 2517.00]\n",
      "Avg_Open_To_Buy                Range: [3.00, 34516.00]\n",
      "Total_Amt_Chng_Q4_Q1           Range: [0.00, 3.40]\n",
      "Total_Trans_Amt                Range: [510.00, 18484.00]\n",
      "Total_Trans_Ct                 Range: [10.00, 139.00]\n",
      "Total_Ct_Chng_Q4_Q1            Range: [0.00, 3.71]\n",
      "Avg_Utilization_Ratio          Range: [0.00, 1.00]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìä Customer_Age:\n",
      "   Range: [26.00, 73.00]\n",
      "   Mean: 46.33\n",
      "   NaN: 0\n",
      "   ‚úÖ Kh√¥ng c√≥ v·∫•n ƒë·ªÅ\n",
      "\n",
      "üìä Dependent_count:\n",
      "   Range: [0.00, 5.00]\n",
      "   Mean: 2.35\n",
      "   NaN: 0\n",
      "   ‚úÖ Kh√¥ng c√≥ v·∫•n ƒë·ªÅ\n",
      "\n",
      "üìä Months_on_book:\n",
      "   Range: [13.00, 56.00]\n",
      "   Mean: 35.93\n",
      "   NaN: 0\n",
      "   ‚úÖ Kh√¥ng c√≥ v·∫•n ƒë·ªÅ\n",
      "\n",
      "üìä Total_Relationship_Count:\n",
      "   Range: [1.00, 6.00]\n",
      "   Mean: 3.81\n",
      "   NaN: 0\n",
      "   ‚úÖ Kh√¥ng c√≥ v·∫•n ƒë·ªÅ\n",
      "\n",
      "üìä Months_Inactive_12_mon:\n",
      "   Range: [0.00, 6.00]\n",
      "   Mean: 2.34\n",
      "   NaN: 0\n",
      "   ‚úÖ Kh√¥ng c√≥ v·∫•n ƒë·ªÅ\n",
      "\n",
      "üìä Contacts_Count_12_mon:\n",
      "   Range: [0.00, 6.00]\n",
      "   Mean: 2.46\n",
      "   NaN: 0\n",
      "   ‚úÖ Kh√¥ng c√≥ v·∫•n ƒë·ªÅ\n",
      "\n",
      "üìä Credit_Limit:\n",
      "   Range: [1438.30, 34516.00]\n",
      "   Mean: 8631.95\n",
      "   NaN: 0\n",
      "   ‚úÖ Kh√¥ng c√≥ v·∫•n ƒë·ªÅ\n",
      "\n",
      "üìä Total_Revolving_Bal:\n",
      "   Range: [0.00, 2517.00]\n",
      "   Mean: 1162.81\n",
      "   NaN: 0\n",
      "   ‚úÖ Kh√¥ng c√≥ v·∫•n ƒë·ªÅ\n",
      "\n",
      "üìä Avg_Open_To_Buy:\n",
      "   Range: [3.00, 34516.00]\n",
      "   Mean: 7469.14\n",
      "   NaN: 0\n",
      "   ‚úÖ Kh√¥ng c√≥ v·∫•n ƒë·ªÅ\n",
      "\n",
      "üìä Total_Amt_Chng_Q4_Q1:\n",
      "   Range: [0.00, 3.40]\n",
      "   Mean: 0.76\n",
      "   NaN: 0\n",
      "   ‚úÖ Kh√¥ng c√≥ v·∫•n ƒë·ªÅ\n",
      "\n",
      "üìä Total_Trans_Amt:\n",
      "   Range: [510.00, 18484.00]\n",
      "   Mean: 4404.09\n",
      "   NaN: 0\n",
      "   ‚úÖ Kh√¥ng c√≥ v·∫•n ƒë·ªÅ\n",
      "\n",
      "üìä Total_Trans_Ct:\n",
      "   Range: [10.00, 139.00]\n",
      "   Mean: 64.86\n",
      "   NaN: 0\n",
      "   ‚úÖ Kh√¥ng c√≥ v·∫•n ƒë·ªÅ\n",
      "\n",
      "üìä Total_Ct_Chng_Q4_Q1:\n",
      "   Range: [0.00, 3.71]\n",
      "   Mean: 0.71\n",
      "   NaN: 0\n",
      "   ‚úÖ Kh√¥ng c√≥ v·∫•n ƒë·ªÅ\n",
      "\n",
      "üìä Avg_Utilization_Ratio:\n",
      "   Range: [0.00, 1.00]\n",
      "   Mean: 0.27\n",
      "   NaN: 0\n",
      "   ‚úÖ Kh√¥ng c√≥ v·∫•n ƒë·ªÅ\n",
      "\n",
      "================================================================================\n",
      "T·ªîNG K·∫æT:\n",
      "S·ªë c·ªôt c√≥ v·∫•n ƒë·ªÅ: 0/14\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Function ƒë·ªÉ validate numerical columns\n",
    "def validate_numerical_column(data, col_idx, col_name, expected_min=None, expected_max=None):\n",
    "    \"\"\"\n",
    "    Ki·ªÉm tra t√≠nh h·ª£p l·ªá c·ªßa c·ªôt numerical\n",
    "    Returns: dict v·ªõi th√¥ng tin validation\n",
    "    \"\"\"\n",
    "    col_data = data[:, col_idx]\n",
    "    \n",
    "    # Lo·∫°i b·ªè NaN ƒë·ªÉ t√≠nh to√°n\n",
    "    valid_data = col_data[~np.isnan(col_data)]\n",
    "    \n",
    "    result = {\n",
    "        'column': col_name,\n",
    "        'total_count': len(col_data),\n",
    "        'nan_count': np.sum(np.isnan(col_data)),\n",
    "        'min': np.min(valid_data) if len(valid_data) > 0 else None,\n",
    "        'max': np.max(valid_data) if len(valid_data) > 0 else None,\n",
    "        'mean': np.mean(valid_data) if len(valid_data) > 0 else None,\n",
    "        'issues': []\n",
    "    }\n",
    "    \n",
    "    # Ki·ªÉm tra NaN\n",
    "    if result['nan_count'] > 0:\n",
    "        result['issues'].append(f\"‚ùå C√≥ {result['nan_count']} gi√° tr·ªã NaN ({result['nan_count']/result['total_count']*100:.2f}%)\")\n",
    "    \n",
    "    # Ki·ªÉm tra gi√° tr·ªã √¢m (n·∫øu kh√¥ng h·ª£p l·ªá)\n",
    "    if len(valid_data) > 0 and expected_min is not None:\n",
    "        if result['min'] < expected_min:\n",
    "            count_invalid = np.sum(valid_data < expected_min)\n",
    "            result['issues'].append(f\"‚ùå C√≥ {count_invalid} gi√° tr·ªã < {expected_min}\")\n",
    "    \n",
    "    # Ki·ªÉm tra gi√° tr·ªã qu√° l·ªõn\n",
    "    if len(valid_data) > 0 and expected_max is not None:\n",
    "        if result['max'] > expected_max:\n",
    "            count_invalid = np.sum(valid_data > expected_max)\n",
    "            result['issues'].append(f\"‚ùå C√≥ {count_invalid} gi√° tr·ªã > {expected_max}\")\n",
    "    \n",
    "    if len(result['issues']) == 0:\n",
    "        result['issues'].append(\"‚úÖ Kh√¥ng c√≥ v·∫•n ƒë·ªÅ\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Validate t·ª´ng numerical column\n",
    "print(\"=\"*80)\n",
    "print(\"KI·ªÇM TRA T√çNH H·ª¢P L·ªÜ C·ª¶A C√ÅC C·ªòT NUMERICAL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# T·ª± ƒë·ªông l·∫•y min/max t·ª´ dataset (thay v√¨ hardcode)\n",
    "print(\"\\nüìä Ph√°t hi·ªán range t·ª± ƒë·ªông t·ª´ dataset:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "validation_rules = []\n",
    "for idx, col_name in enumerate(numerical_cols):\n",
    "    col_data = numerical_data[:, idx]\n",
    "    valid_data = col_data[~np.isnan(col_data)]\n",
    "    \n",
    "    # L·∫•y actual min/max t·ª´ data\n",
    "    actual_min = np.min(valid_data)\n",
    "    actual_max = np.max(valid_data)\n",
    "    \n",
    "    # ƒê·ªãnh nghƒ©a expected bounds d·ª±a tr√™n business logic\n",
    "    # Ch·ªâ check c√°c constraints logic (kh√¥ng √¢m, ratio 0-1, etc.)\n",
    "    if col_name == 'Avg_Utilization_Ratio':\n",
    "        expected_min, expected_max = 0, 1  # Ratio ph·∫£i 0-1\n",
    "    elif col_name in ['Total_Revolving_Bal', 'Avg_Open_To_Buy', 'Credit_Limit', \n",
    "                      'Total_Trans_Amt', 'Total_Trans_Ct']:\n",
    "        expected_min, expected_max = 0, None  # Kh√¥ng ƒë∆∞·ª£c √¢m\n",
    "    elif col_name in ['Months_Inactive_12_mon', 'Contacts_Count_12_mon']:\n",
    "        expected_min, expected_max = 0, None  # Kh√¥ng ƒë∆∞·ª£c √¢m\n",
    "    elif col_name == 'Customer_Age':\n",
    "        expected_min, expected_max = 18, None  # Tu·ªïi t·ªëi thi·ªÉu 18\n",
    "    elif col_name == 'Total_Relationship_Count':\n",
    "        expected_min, expected_max = 1, None  # √çt nh·∫•t 1 s·∫£n ph·∫©m\n",
    "    else:\n",
    "        expected_min, expected_max = None, None  # Kh√¥ng c√≥ constraint\n",
    "    \n",
    "    validation_rules.append((idx, col_name, expected_min, expected_max))\n",
    "    \n",
    "    print(f\"{col_name:<30} Range: [{actual_min:.2f}, {actual_max:.2f}]\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "\n",
    "validation_results = []\n",
    "for idx, col_name, min_val, max_val in validation_rules:\n",
    "    result = validate_numerical_column(numerical_data, idx, col_name, min_val, max_val)\n",
    "    validation_results.append(result)\n",
    "    \n",
    "    print(f\"\\nüìä {col_name}:\")\n",
    "    print(f\"   Range: [{result['min']:.2f}, {result['max']:.2f}]\")\n",
    "    print(f\"   Mean: {result['mean']:.2f}\")\n",
    "    print(f\"   NaN: {result['nan_count']}\")\n",
    "    for issue in result['issues']:\n",
    "        print(f\"   {issue}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"T·ªîNG K·∫æT:\")\n",
    "total_issues = sum(1 for r in validation_results if len([i for i in r['issues'] if '‚ùå' in i]) > 0)\n",
    "print(f\"S·ªë c·ªôt c√≥ v·∫•n ƒë·ªÅ: {total_issues}/{len(validation_results)}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4d8714d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "KI·ªÇM TRA T√çNH H·ª¢P L·ªÜ C·ª¶A C√ÅC C·ªòT CATEGORICAL\n",
      "================================================================================\n",
      "\n",
      "üìã Gender:\n",
      "   S·ªë gi√° tr·ªã unique: 2\n",
      "   C√°c gi√° tr·ªã:\n",
      "      - 'F': 5,358 (52.91%)\n",
      "      - 'M': 4,769 (47.09%)\n",
      "\n",
      "üìã Education_Level:\n",
      "   S·ªë gi√° tr·ªã unique: 7\n",
      "   C√°c gi√° tr·ªã:\n",
      "      - 'College': 1,013 (10.00%)\n",
      "      - 'Doctorate': 451 (4.45%)\n",
      "      - 'Graduate': 3,128 (30.89%)\n",
      "      - 'High School': 2,013 (19.88%)\n",
      "      - 'Post-Graduate': 516 (5.10%)\n",
      "      - 'Uneducated': 1,487 (14.68%)\n",
      "      - 'Unknown': 1,519 (15.00%)\n",
      "   ‚ö†Ô∏è C√≥ 1,519 gi√° tr·ªã 'Unknown' (15.00%)\n",
      "\n",
      "üìã Marital_Status:\n",
      "   S·ªë gi√° tr·ªã unique: 4\n",
      "   C√°c gi√° tr·ªã:\n",
      "      - 'Divorced': 748 (7.39%)\n",
      "      - 'Married': 4,687 (46.28%)\n",
      "      - 'Single': 3,943 (38.94%)\n",
      "      - 'Unknown': 749 (7.40%)\n",
      "   ‚ö†Ô∏è C√≥ 749 gi√° tr·ªã 'Unknown' (7.40%)\n",
      "\n",
      "üìã Income_Category:\n",
      "   S·ªë gi√° tr·ªã unique: 6\n",
      "   C√°c gi√° tr·ªã:\n",
      "      - '$120K +': 727 (7.18%)\n",
      "      - '$40K - $60K': 1,790 (17.68%)\n",
      "      - '$60K - $80K': 1,402 (13.84%)\n",
      "      - '$80K - $120K': 1,535 (15.16%)\n",
      "      - 'Less than $40K': 3,561 (35.16%)\n",
      "      - 'Unknown': 1,112 (10.98%)\n",
      "   ‚ö†Ô∏è C√≥ 1,112 gi√° tr·ªã 'Unknown' (10.98%)\n",
      "\n",
      "üìã Card_Category:\n",
      "   S·ªë gi√° tr·ªã unique: 4\n",
      "   C√°c gi√° tr·ªã:\n",
      "      - 'Blue': 9,436 (93.18%)\n",
      "      - 'Gold': 116 (1.15%)\n",
      "      - 'Platinum': 20 (0.20%)\n",
      "      - 'Silver': 555 (5.48%)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Validate categorical columns\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KI·ªÇM TRA T√çNH H·ª¢P L·ªÜ C·ª¶A C√ÅC C·ªòT CATEGORICAL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx, col_name in enumerate(categorical_cols):\n",
    "    col_data = categorical_data[:, idx]\n",
    "    unique_values = np.unique(col_data)\n",
    "    \n",
    "    print(f\"\\nüìã {col_name}:\")\n",
    "    print(f\"   S·ªë gi√° tr·ªã unique: {len(unique_values)}\")\n",
    "    print(f\"   C√°c gi√° tr·ªã:\")\n",
    "    \n",
    "    for val in unique_values:\n",
    "        count = np.sum(col_data == val)\n",
    "        pct = count / len(col_data) * 100\n",
    "        print(f\"      - '{val}': {count:,} ({pct:.2f}%)\")\n",
    "    \n",
    "    # Ki·ªÉm tra missing values ƒë∆∞·ª£c encode l√† 'Unknown'\n",
    "    if 'Unknown' in unique_values:\n",
    "        unknown_count = np.sum(col_data == 'Unknown')\n",
    "        print(f\"   ‚ö†Ô∏è C√≥ {unknown_count:,} gi√° tr·ªã 'Unknown' ({unknown_count/len(col_data)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e9db189f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PH√ÅT HI·ªÜN OUTLIERS - PH∆Ø∆†NG PH√ÅP IQR (1.5*IQR)\n",
      "================================================================================\n",
      "\n",
      "üìä Customer_Age:\n",
      "   Q1=41.00, Q3=52.00, IQR=11.00\n",
      "   Bounds: [24.50, 68.50]\n",
      "   Outliers th·∫•p: 0\n",
      "   Outliers cao: 2\n",
      "   ‚ö†Ô∏è T·ªïng outliers: 2 (0.02%)\n",
      "\n",
      "üìä Months_on_book:\n",
      "   Q1=31.00, Q3=40.00, IQR=9.00\n",
      "   Bounds: [17.50, 53.50]\n",
      "   Outliers th·∫•p: 188\n",
      "   Outliers cao: 198\n",
      "   ‚ö†Ô∏è T·ªïng outliers: 386 (3.81%)\n",
      "\n",
      "üìä Months_Inactive_12_mon:\n",
      "   Q1=2.00, Q3=3.00, IQR=1.00\n",
      "   Bounds: [0.50, 4.50]\n",
      "   Outliers th·∫•p: 29\n",
      "   Outliers cao: 302\n",
      "   ‚ö†Ô∏è T·ªïng outliers: 331 (3.27%)\n",
      "\n",
      "üìä Contacts_Count_12_mon:\n",
      "   Q1=2.00, Q3=3.00, IQR=1.00\n",
      "   Bounds: [0.50, 4.50]\n",
      "   Outliers th·∫•p: 399\n",
      "   Outliers cao: 230\n",
      "   ‚ö†Ô∏è T·ªïng outliers: 629 (6.21%)\n",
      "\n",
      "üìä Credit_Limit:\n",
      "   Q1=2555.00, Q3=11067.50, IQR=8512.50\n",
      "   Bounds: [-10213.75, 23836.25]\n",
      "   Outliers th·∫•p: 0\n",
      "   Outliers cao: 984\n",
      "   ‚ö†Ô∏è T·ªïng outliers: 984 (9.72%)\n",
      "\n",
      "üìä Avg_Open_To_Buy:\n",
      "   Q1=1324.50, Q3=9859.00, IQR=8534.50\n",
      "   Bounds: [-11477.25, 22660.75]\n",
      "   Outliers th·∫•p: 0\n",
      "   Outliers cao: 963\n",
      "   ‚ö†Ô∏è T·ªïng outliers: 963 (9.51%)\n",
      "\n",
      "üìä Total_Amt_Chng_Q4_Q1:\n",
      "   Q1=0.63, Q3=0.86, IQR=0.23\n",
      "   Bounds: [0.29, 1.20]\n",
      "   Outliers th·∫•p: 48\n",
      "   Outliers cao: 348\n",
      "   ‚ö†Ô∏è T·ªïng outliers: 396 (3.91%)\n",
      "\n",
      "üìä Total_Trans_Amt:\n",
      "   Q1=2155.50, Q3=4741.00, IQR=2585.50\n",
      "   Bounds: [-1722.75, 8619.25]\n",
      "   Outliers th·∫•p: 0\n",
      "   Outliers cao: 896\n",
      "   ‚ö†Ô∏è T·ªïng outliers: 896 (8.85%)\n",
      "\n",
      "üìä Total_Trans_Ct:\n",
      "   Q1=45.00, Q3=81.00, IQR=36.00\n",
      "   Bounds: [-9.00, 135.00]\n",
      "   Outliers th·∫•p: 0\n",
      "   Outliers cao: 2\n",
      "   ‚ö†Ô∏è T·ªïng outliers: 2 (0.02%)\n",
      "\n",
      "üìä Total_Ct_Chng_Q4_Q1:\n",
      "   Q1=0.58, Q3=0.82, IQR=0.24\n",
      "   Bounds: [0.23, 1.17]\n",
      "   Outliers th·∫•p: 96\n",
      "   Outliers cao: 298\n",
      "   ‚ö†Ô∏è T·ªïng outliers: 394 (3.89%)\n",
      "\n",
      "================================================================================\n",
      "PH√ÅT HI·ªÜN OUTLIERS - PH∆Ø∆†NG PH√ÅP Z-SCORE (threshold=3)\n",
      "================================================================================\n",
      "\n",
      "üìä Customer_Age:\n",
      "   Mean=46.33, Std=8.02\n",
      "   ‚ö†Ô∏è Outliers: 1 (0.01%)\n",
      "\n",
      "üìä Months_Inactive_12_mon:\n",
      "   Mean=2.34, Std=1.01\n",
      "   ‚ö†Ô∏è Outliers: 124 (1.22%)\n",
      "\n",
      "üìä Contacts_Count_12_mon:\n",
      "   Mean=2.46, Std=1.11\n",
      "   ‚ö†Ô∏è Outliers: 54 (0.53%)\n",
      "\n",
      "üìä Total_Amt_Chng_Q4_Q1:\n",
      "   Mean=0.76, Std=0.22\n",
      "   ‚ö†Ô∏è Outliers: 163 (1.61%)\n",
      "\n",
      "üìä Total_Trans_Amt:\n",
      "   Mean=4404.09, Std=3396.96\n",
      "   ‚ö†Ô∏è Outliers: 391 (3.86%)\n",
      "\n",
      "üìä Total_Trans_Ct:\n",
      "   Mean=64.86, Std=23.47\n",
      "   ‚ö†Ô∏è Outliers: 2 (0.02%)\n",
      "\n",
      "üìä Total_Ct_Chng_Q4_Q1:\n",
      "   Mean=0.71, Std=0.24\n",
      "   ‚ö†Ô∏è Outliers: 113 (1.12%)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Function ƒë·ªÉ detect outliers b·∫±ng IQR method\n",
    "def detect_outliers_iqr(data, col_idx, col_name, multiplier=1.5):\n",
    "    \"\"\"\n",
    "    Ph√°t hi·ªán outliers b·∫±ng ph∆∞∆°ng ph√°p IQR\n",
    "    Outlier n·∫øu: value < Q1 - multiplier*IQR ho·∫∑c value > Q3 + multiplier*IQR\n",
    "    \"\"\"\n",
    "    col_data = data[:, col_idx]\n",
    "    \n",
    "    # T√≠nh Q1, Q3, IQR\n",
    "    q1 = np.percentile(col_data, 25)\n",
    "    q3 = np.percentile(col_data, 75)\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    # T√≠nh boundaries\n",
    "    lower_bound = q1 - multiplier * iqr\n",
    "    upper_bound = q3 + multiplier * iqr\n",
    "    \n",
    "    # T√¨m outliers\n",
    "    outliers_lower = col_data < lower_bound\n",
    "    outliers_upper = col_data > upper_bound\n",
    "    outliers = outliers_lower | outliers_upper\n",
    "    \n",
    "    return {\n",
    "        'column': col_name,\n",
    "        'q1': q1,\n",
    "        'q3': q3,\n",
    "        'iqr': iqr,\n",
    "        'lower_bound': lower_bound,\n",
    "        'upper_bound': upper_bound,\n",
    "        'n_outliers_lower': np.sum(outliers_lower),\n",
    "        'n_outliers_upper': np.sum(outliers_upper),\n",
    "        'n_outliers_total': np.sum(outliers),\n",
    "        'outlier_pct': np.sum(outliers) / len(col_data) * 100,\n",
    "        'outlier_indices': np.where(outliers)[0]\n",
    "    }\n",
    "\n",
    "# Function ƒë·ªÉ detect outliers b·∫±ng Z-score method\n",
    "def detect_outliers_zscore(data, col_idx, col_name, threshold=3):\n",
    "    \"\"\"\n",
    "    Ph√°t hi·ªán outliers b·∫±ng ph∆∞∆°ng ph√°p Z-score\n",
    "    Outlier n·∫øu: |z-score| > threshold\n",
    "    \"\"\"\n",
    "    col_data = data[:, col_idx]\n",
    "    \n",
    "    mean = np.mean(col_data)\n",
    "    std = np.std(col_data)\n",
    "    \n",
    "    # T√≠nh z-scores\n",
    "    z_scores = np.abs((col_data - mean) / std)\n",
    "    \n",
    "    # T√¨m outliers\n",
    "    outliers = z_scores > threshold\n",
    "    \n",
    "    return {\n",
    "        'column': col_name,\n",
    "        'mean': mean,\n",
    "        'std': std,\n",
    "        'threshold': threshold,\n",
    "        'n_outliers': np.sum(outliers),\n",
    "        'outlier_pct': np.sum(outliers) / len(col_data) * 100,\n",
    "        'outlier_indices': np.where(outliers)[0]\n",
    "    }\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PH√ÅT HI·ªÜN OUTLIERS - PH∆Ø∆†NG PH√ÅP IQR (1.5*IQR)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "outlier_results_iqr = []\n",
    "for idx, col_name in enumerate(numerical_cols):\n",
    "    result = detect_outliers_iqr(numerical_data, idx, col_name, multiplier=1.5)\n",
    "    outlier_results_iqr.append(result)\n",
    "    \n",
    "    if result['n_outliers_total'] > 0:\n",
    "        print(f\"\\nüìä {col_name}:\")\n",
    "        print(f\"   Q1={result['q1']:.2f}, Q3={result['q3']:.2f}, IQR={result['iqr']:.2f}\")\n",
    "        print(f\"   Bounds: [{result['lower_bound']:.2f}, {result['upper_bound']:.2f}]\")\n",
    "        print(f\"   Outliers th·∫•p: {result['n_outliers_lower']}\")\n",
    "        print(f\"   Outliers cao: {result['n_outliers_upper']}\")\n",
    "        print(f\"   ‚ö†Ô∏è T·ªïng outliers: {result['n_outliers_total']} ({result['outlier_pct']:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PH√ÅT HI·ªÜN OUTLIERS - PH∆Ø∆†NG PH√ÅP Z-SCORE (threshold=3)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "outlier_results_zscore = []\n",
    "for idx, col_name in enumerate(numerical_cols):\n",
    "    result = detect_outliers_zscore(numerical_data, idx, col_name, threshold=3)\n",
    "    outlier_results_zscore.append(result)\n",
    "    \n",
    "    if result['n_outliers'] > 0:\n",
    "        print(f\"\\nüìä {col_name}:\")\n",
    "        print(f\"   Mean={result['mean']:.2f}, Std={result['std']:.2f}\")\n",
    "        print(f\"   ‚ö†Ô∏è Outliers: {result['n_outliers']} ({result['outlier_pct']:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9ec27a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "KI·ªÇM TRA MISSING VALUES\n",
      "================================================================================\n",
      "\n",
      "üìä Numerical Data:\n",
      "  Customer_Age: 0 NaN values\n",
      "  Dependent_count: 0 NaN values\n",
      "  Months_on_book: 0 NaN values\n",
      "  Total_Relationship_Count: 0 NaN values\n",
      "  Months_Inactive_12_mon: 0 NaN values\n",
      "  Contacts_Count_12_mon: 0 NaN values\n",
      "  Credit_Limit: 0 NaN values\n",
      "  Total_Revolving_Bal: 0 NaN values\n",
      "  Avg_Open_To_Buy: 0 NaN values\n",
      "  Total_Amt_Chng_Q4_Q1: 0 NaN values\n",
      "  Total_Trans_Amt: 0 NaN values\n",
      "  Total_Trans_Ct: 0 NaN values\n",
      "  Total_Ct_Chng_Q4_Q1: 0 NaN values\n",
      "  Avg_Utilization_Ratio: 0 NaN values\n",
      "\n",
      "üìã Categorical Data:\n",
      "  Gender: 0 missing values\n",
      "  Education_Level: 1519 'Unknown' values (15.00%)\n",
      "  Marital_Status: 749 'Unknown' values (7.40%)\n",
      "  Income_Category: 1112 'Unknown' values (10.98%)\n",
      "  Card_Category: 0 missing values\n"
     ]
    }
   ],
   "source": [
    "# X√°c nh·∫≠n kh√¥ng c√≥ missing values trong numerical data\n",
    "print(\"=\"*80)\n",
    "print(\"KI·ªÇM TRA MISSING VALUES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä Numerical Data:\")\n",
    "for idx, col_name in enumerate(numerical_cols):\n",
    "    nan_count = np.sum(np.isnan(numerical_data[:, idx]))\n",
    "    print(f\"  {col_name}: {nan_count} NaN values\")\n",
    "\n",
    "print(\"\\nüìã Categorical Data:\")\n",
    "for idx, col_name in enumerate(categorical_cols):\n",
    "    unknown_count = np.sum(categorical_data[:, idx] == 'Unknown')\n",
    "    if unknown_count > 0:\n",
    "        print(f\"  {col_name}: {unknown_count} 'Unknown' values ({unknown_count/len(categorical_data)*100:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"  {col_name}: 0 missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5ced9b",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß B∆Ø·ªöC 5: FEATURE ENGINEERING\n",
    "\n",
    "D·ª±a tr√™n insights t·ª´ EDA, t·∫°o c√°c features m·ªõi:\n",
    "1. **Lo·∫°i b·ªè multicollinear feature**: `Avg_Open_To_Buy` (r=0.996 v·ªõi Credit_Limit)\n",
    "2. **T·∫°o features m·ªõi** t·ª´ existing features\n",
    "3. **Encode categorical variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "842f8693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LO·∫†I B·ªé MULTICOLLINEAR FEATURE\n",
      "================================================================================\n",
      "\n",
      "‚ùå Lo·∫°i b·ªè: Avg_Open_To_Buy (correlation 0.996 v·ªõi Credit_Limit)\n",
      "‚úÖ Shape tr∆∞·ªõc: (10127, 14)\n",
      "‚úÖ Shape sau: (10127, 13)\n",
      "‚úÖ S·ªë features c√≤n l·∫°i: 13\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 5.1: LO·∫†I B·ªé MULTICOLLINEAR FEATURE\n",
    "# ============================================================================\n",
    "\n",
    "# Avg_Open_To_Buy c√≥ correlation 0.996 v·ªõi Credit_Limit -> lo·∫°i b·ªè\n",
    "# Index c·ªßa Avg_Open_To_Buy trong numerical_data l√† 8\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LO·∫†I B·ªé MULTICOLLINEAR FEATURE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "avg_open_to_buy_idx = numerical_cols.index('Avg_Open_To_Buy')\n",
    "print(f\"\\n‚ùå Lo·∫°i b·ªè: {numerical_cols[avg_open_to_buy_idx]} (correlation 0.996 v·ªõi Credit_Limit)\")\n",
    "\n",
    "# T·∫°o numerical_data m·ªõi kh√¥ng c√≥ Avg_Open_To_Buy\n",
    "indices_to_keep = [i for i in range(numerical_data.shape[1]) if i != avg_open_to_buy_idx]\n",
    "numerical_data_clean = numerical_data[:, indices_to_keep]\n",
    "numerical_cols_clean = [col for i, col in enumerate(numerical_cols) if i != avg_open_to_buy_idx]\n",
    "\n",
    "print(f\"‚úÖ Shape tr∆∞·ªõc: {numerical_data.shape}\")\n",
    "print(f\"‚úÖ Shape sau: {numerical_data_clean.shape}\")\n",
    "print(f\"‚úÖ S·ªë features c√≤n l·∫°i: {len(numerical_cols_clean)}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cd2ae29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "T·∫†O C√ÅC FEATURES M·ªöI\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Feature 1: Avg_Transaction_Value\n",
      "   Range: [19.14, 190.19]\n",
      "   Mean: 62.61\n",
      "\n",
      "‚úÖ Feature 2: Credit_Utilization_Ratio\n",
      "   Range: [0.00, 1.00]\n",
      "   Mean: 0.27\n",
      "\n",
      "‚úÖ Feature 3: Transaction_Frequency (trans per month)\n",
      "   Range: [0.19, 9.77]\n",
      "   Mean: 1.92\n",
      "\n",
      "‚úÖ Feature 4: Activity_Score\n",
      "   Range: [7.50, 120.08]\n",
      "   Mean: 52.29\n",
      "\n",
      "‚úÖ Feature 5: Relationship_Age_Ratio (products per year)\n",
      "   Range: [0.21, 5.54]\n",
      "   Mean: 1.36\n",
      "\n",
      "‚úÖ Feature 6: Age_When_Joined\n",
      "   Range: [23.00, 70.00]\n",
      "   Mean: 43.33\n",
      "\n",
      "================================================================================\n",
      "‚úÖ ƒê√£ t·∫°o 6 features m·ªõi\n",
      "‚úÖ T·ªïng s·ªë numerical features: 19\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 5.2: T·∫†O C√ÅC FEATURES M·ªöI\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"T·∫†O C√ÅC FEATURES M·ªöI\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Helper function ƒë·ªÉ tr√°nh chia cho 0\n",
    "def safe_divide(numerator, denominator, default=0):\n",
    "    \"\"\"Chia an to√†n, tr·∫£ v·ªÅ default n·∫øu denominator = 0\"\"\"\n",
    "    result = np.zeros_like(numerator, dtype=float)\n",
    "    mask = denominator != 0\n",
    "    result[mask] = numerator[mask] / denominator[mask]\n",
    "    result[~mask] = default\n",
    "    return result\n",
    "\n",
    "# Get indices c·ªßa c√°c columns c·∫ßn thi·∫øt\n",
    "def get_col_idx(col_name):\n",
    "    return numerical_cols_clean.index(col_name)\n",
    "\n",
    "# Feature 1: Avg_Transaction_Value = Total_Trans_Amt / Total_Trans_Ct\n",
    "trans_amt = numerical_data_clean[:, get_col_idx('Total_Trans_Amt')]\n",
    "trans_ct = numerical_data_clean[:, get_col_idx('Total_Trans_Ct')]\n",
    "avg_trans_value = safe_divide(trans_amt, trans_ct, default=0)\n",
    "print(\"\\n‚úÖ Feature 1: Avg_Transaction_Value\")\n",
    "print(f\"   Range: [{np.min(avg_trans_value):.2f}, {np.max(avg_trans_value):.2f}]\")\n",
    "print(f\"   Mean: {np.mean(avg_trans_value):.2f}\")\n",
    "\n",
    "# Feature 2: Credit_Utilization_Ratio = Total_Revolving_Bal / Credit_Limit\n",
    "revolving_bal = numerical_data_clean[:, get_col_idx('Total_Revolving_Bal')]\n",
    "credit_limit = numerical_data_clean[:, get_col_idx('Credit_Limit')]\n",
    "credit_util = safe_divide(revolving_bal, credit_limit, default=0)\n",
    "print(\"\\n‚úÖ Feature 2: Credit_Utilization_Ratio\")\n",
    "print(f\"   Range: [{np.min(credit_util):.2f}, {np.max(credit_util):.2f}]\")\n",
    "print(f\"   Mean: {np.mean(credit_util):.2f}\")\n",
    "\n",
    "# Feature 3: Transaction_Frequency = Total_Trans_Ct / Months_on_book\n",
    "months_on_book = numerical_data_clean[:, get_col_idx('Months_on_book')]\n",
    "trans_frequency = safe_divide(trans_ct, months_on_book, default=0)\n",
    "print(\"\\n‚úÖ Feature 3: Transaction_Frequency (trans per month)\")\n",
    "print(f\"   Range: [{np.min(trans_frequency):.2f}, {np.max(trans_frequency):.2f}]\")\n",
    "print(f\"   Mean: {np.mean(trans_frequency):.2f}\")\n",
    "\n",
    "# Feature 4: Activity_Score = Total_Trans_Ct * (1 - Months_Inactive_12_mon/12)\n",
    "months_inactive = numerical_data_clean[:, get_col_idx('Months_Inactive_12_mon')]\n",
    "activity_score = trans_ct * (1 - months_inactive / 12)\n",
    "print(\"\\n‚úÖ Feature 4: Activity_Score\")\n",
    "print(f\"   Range: [{np.min(activity_score):.2f}, {np.max(activity_score):.2f}]\")\n",
    "print(f\"   Mean: {np.mean(activity_score):.2f}\")\n",
    "\n",
    "# Feature 5: Relationship_Age_Ratio = Total_Relationship_Count / (Months_on_book/12)\n",
    "relationship_count = numerical_data_clean[:, get_col_idx('Total_Relationship_Count')]\n",
    "years_on_book = months_on_book / 12\n",
    "relationship_age_ratio = safe_divide(relationship_count, years_on_book, default=0)\n",
    "print(\"\\n‚úÖ Feature 5: Relationship_Age_Ratio (products per year)\")\n",
    "print(f\"   Range: [{np.min(relationship_age_ratio):.2f}, {np.max(relationship_age_ratio):.2f}]\")\n",
    "print(f\"   Mean: {np.mean(relationship_age_ratio):.2f}\")\n",
    "\n",
    "# Feature 6: Customer_Age_Normalized (tu·ªïi so v·ªõi th·ªùi gian quan h·ªá)\n",
    "customer_age = numerical_data_clean[:, get_col_idx('Customer_Age')]\n",
    "age_when_joined = customer_age - (months_on_book / 12)\n",
    "print(\"\\n‚úÖ Feature 6: Age_When_Joined\")\n",
    "print(f\"   Range: [{np.min(age_when_joined):.2f}, {np.max(age_when_joined):.2f}]\")\n",
    "print(f\"   Mean: {np.mean(age_when_joined):.2f}\")\n",
    "\n",
    "# G·ªôp t·∫•t c·∫£ features m·ªõi v√†o array\n",
    "new_features = np.column_stack([\n",
    "    avg_trans_value,\n",
    "    credit_util,\n",
    "    trans_frequency,\n",
    "    activity_score,\n",
    "    relationship_age_ratio,\n",
    "    age_when_joined\n",
    "])\n",
    "\n",
    "new_feature_names = [\n",
    "    'Avg_Transaction_Value',\n",
    "    'Credit_Utilization_Ratio',\n",
    "    'Transaction_Frequency',\n",
    "    'Activity_Score',\n",
    "    'Relationship_Age_Ratio',\n",
    "    'Age_When_Joined'\n",
    "]\n",
    "\n",
    "# K·∫øt h·ª£p v·ªõi numerical data g·ªëc\n",
    "numerical_data_engineered = np.column_stack([numerical_data_clean, new_features])\n",
    "numerical_cols_engineered = numerical_cols_clean + new_feature_names\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úÖ ƒê√£ t·∫°o {len(new_feature_names)} features m·ªõi\")\n",
    "print(f\"‚úÖ T·ªïng s·ªë numerical features: {numerical_data_engineered.shape[1]}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0bf57e",
   "metadata": {},
   "source": [
    "---\n",
    "## üî¢ B∆Ø·ªöC 6: CHU·∫®N H√ìA (NORMALIZATION) & ƒêI·ªÄU CHU·∫®N (STANDARDIZATION)\n",
    "\n",
    "### Chi·∫øn l∆∞·ª£c:\n",
    "1. **Normalization (Min-Max, Log Transform)**: Cho features c√≥ distribution kh√¥ng Gaussian ho·∫∑c c√≥ outliers\n",
    "2. **Standardization (Z-score)**: Cho features c√≥ distribution g·∫ßn Gaussian\n",
    "\n",
    "Ki·ªÉm tra distribution c·ªßa t·ª´ng feature ƒë·ªÉ quy·∫øt ƒë·ªãnh ph∆∞∆°ng ph√°p ph√π h·ª£p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "de3c809a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PH√ÇN T√çCH DISTRIBUTION C·ª¶A C√ÅC FEATURES\n",
      "================================================================================\n",
      "\n",
      "üìä Customer_Age:\n",
      "   Skewness: -0.034\n",
      "   Kurtosis: -0.289\n",
      "   Distribution: Gaussian-like\n",
      "   ‚û°Ô∏è Recommended: Standardization (Z-score)\n",
      "\n",
      "üìä Dependent_count:\n",
      "   Skewness: -0.021\n",
      "   Kurtosis: -0.683\n",
      "   Distribution: Gaussian-like\n",
      "   ‚û°Ô∏è Recommended: Standardization (Z-score)\n",
      "\n",
      "üìä Months_on_book:\n",
      "   Skewness: -0.107\n",
      "   Kurtosis: 0.399\n",
      "   Distribution: Gaussian-like\n",
      "   ‚û°Ô∏è Recommended: Standardization (Z-score)\n",
      "\n",
      "üìä Total_Relationship_Count:\n",
      "   Skewness: -0.162\n",
      "   Kurtosis: -1.006\n",
      "   Distribution: Gaussian-like\n",
      "   ‚û°Ô∏è Recommended: Standardization (Z-score)\n",
      "\n",
      "üìä Months_Inactive_12_mon:\n",
      "   Skewness: 0.633\n",
      "   Kurtosis: 1.097\n",
      "   Distribution: Moderate skew\n",
      "   ‚û°Ô∏è Recommended: Normalization (Min-Max)\n",
      "\n",
      "üìä Contacts_Count_12_mon:\n",
      "   Skewness: 0.011\n",
      "   Kurtosis: 0.000\n",
      "   Distribution: Gaussian-like\n",
      "   ‚û°Ô∏è Recommended: Standardization (Z-score)\n",
      "\n",
      "üìä Credit_Limit:\n",
      "   Skewness: 1.666\n",
      "   Kurtosis: 1.808\n",
      "   Distribution: Highly skewed\n",
      "   ‚û°Ô∏è Recommended: Log Transform + Standardization\n",
      "\n",
      "üìä Total_Revolving_Bal:\n",
      "   Skewness: -0.149\n",
      "   Kurtosis: -1.146\n",
      "   Distribution: Gaussian-like\n",
      "   ‚û°Ô∏è Recommended: Standardization (Z-score)\n",
      "\n",
      "üìä Total_Amt_Chng_Q4_Q1:\n",
      "   Skewness: 1.732\n",
      "   Kurtosis: 9.988\n",
      "   Distribution: Highly skewed\n",
      "   ‚û°Ô∏è Recommended: Log Transform + Standardization\n",
      "\n",
      "üìä Total_Trans_Amt:\n",
      "   Skewness: 2.041\n",
      "   Kurtosis: 3.892\n",
      "   Distribution: Highly skewed\n",
      "   ‚û°Ô∏è Recommended: Log Transform + Standardization\n",
      "\n",
      "üìä Total_Trans_Ct:\n",
      "   Skewness: 0.154\n",
      "   Kurtosis: -0.368\n",
      "   Distribution: Gaussian-like\n",
      "   ‚û°Ô∏è Recommended: Standardization (Z-score)\n",
      "\n",
      "üìä Total_Ct_Chng_Q4_Q1:\n",
      "   Skewness: 2.064\n",
      "   Kurtosis: 15.681\n",
      "   Distribution: Highly skewed\n",
      "   ‚û°Ô∏è Recommended: Log Transform + Standardization\n",
      "\n",
      "üìä Avg_Utilization_Ratio:\n",
      "   Skewness: 0.718\n",
      "   Kurtosis: -0.795\n",
      "   Distribution: Moderate skew\n",
      "   ‚û°Ô∏è Recommended: Normalization (Min-Max)\n",
      "\n",
      "üìä Avg_Transaction_Value:\n",
      "   Skewness: 1.883\n",
      "   Kurtosis: 3.315\n",
      "   Distribution: Highly skewed\n",
      "   ‚û°Ô∏è Recommended: Log Transform + Standardization\n",
      "\n",
      "üìä Credit_Utilization_Ratio:\n",
      "   Skewness: 0.718\n",
      "   Kurtosis: -0.795\n",
      "   Distribution: Moderate skew\n",
      "   ‚û°Ô∏è Recommended: Normalization (Min-Max)\n",
      "\n",
      "üìä Transaction_Frequency:\n",
      "   Skewness: 1.442\n",
      "   Kurtosis: 5.380\n",
      "   Distribution: Highly skewed\n",
      "   ‚û°Ô∏è Recommended: Log Transform + Standardization\n",
      "\n",
      "üìä Activity_Score:\n",
      "   Skewness: 0.310\n",
      "   Kurtosis: -0.205\n",
      "   Distribution: Gaussian-like\n",
      "   ‚û°Ô∏è Recommended: Standardization (Z-score)\n",
      "\n",
      "üìä Relationship_Age_Ratio:\n",
      "   Skewness: 1.181\n",
      "   Kurtosis: 3.237\n",
      "   Distribution: Highly skewed\n",
      "   ‚û°Ô∏è Recommended: Log Transform + Standardization\n",
      "\n",
      "üìä Age_When_Joined:\n",
      "   Skewness: -0.038\n",
      "   Kurtosis: -0.273\n",
      "   Distribution: Gaussian-like\n",
      "   ‚û°Ô∏è Recommended: Standardization (Z-score)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 6.1: KI·ªÇM TRA DISTRIBUTION (SKEWNESS)\n",
    "# ============================================================================\n",
    "\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PH√ÇN T√çCH DISTRIBUTION C·ª¶A C√ÅC FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "skewness_results = []\n",
    "for idx, col_name in enumerate(numerical_cols_engineered):\n",
    "    col_data = numerical_data_engineered[:, idx]\n",
    "    \n",
    "    # T√≠nh skewness v√† kurtosis\n",
    "    skewness = skew(col_data)\n",
    "    kurt = kurtosis(col_data)\n",
    "    \n",
    "    # Ph√¢n lo·∫°i distribution\n",
    "    if abs(skewness) < 0.5:\n",
    "        dist_type = \"Gaussian-like\"\n",
    "        method = \"Standardization (Z-score)\"\n",
    "    elif abs(skewness) < 1.0:\n",
    "        dist_type = \"Moderate skew\"\n",
    "        method = \"Normalization (Min-Max)\"\n",
    "    else:\n",
    "        dist_type = \"Highly skewed\"\n",
    "        method = \"Log Transform + Standardization\"\n",
    "    \n",
    "    skewness_results.append({\n",
    "        'column': col_name,\n",
    "        'skewness': skewness,\n",
    "        'kurtosis': kurt,\n",
    "        'distribution': dist_type,\n",
    "        'recommended_method': method\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nüìä {col_name}:\")\n",
    "    print(f\"   Skewness: {skewness:.3f}\")\n",
    "    print(f\"   Kurtosis: {kurt:.3f}\")\n",
    "    print(f\"   Distribution: {dist_type}\")\n",
    "    print(f\"   ‚û°Ô∏è Recommended: {method}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f92a0024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a c√°c h√†m normalization & standardization:\n",
      "   1. min_max_normalization() - Scale to [0,1]\n",
      "   2. log_transformation() - Log transform\n",
      "   3. decimal_scaling() - Decimal scaling\n",
      "   4. standardization() - Z-score (mean=0, std=1)\n",
      "   5. robust_scaling() - Robust v·ªõi outliers\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 6.2: IMPLEMENT NORMALIZATION & STANDARDIZATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def min_max_normalization(data, feature_min=None, feature_max=None):\n",
    "    \"\"\"\n",
    "    Min-Max Normalization: scale to [0, 1]\n",
    "    X_norm = (X - X_min) / (X_max - X_min)\n",
    "    \"\"\"\n",
    "    if feature_min is None:\n",
    "        feature_min = np.min(data)\n",
    "    if feature_max is None:\n",
    "        feature_max = np.max(data)\n",
    "    \n",
    "    # Tr√°nh chia cho 0\n",
    "    if feature_max - feature_min == 0:\n",
    "        return np.zeros_like(data)\n",
    "    \n",
    "    return (data - feature_min) / (feature_max - feature_min)\n",
    "\n",
    "def log_transformation(data, offset=1):\n",
    "    \"\"\"\n",
    "    Log Transformation: log(X + offset)\n",
    "    Offset ƒë·ªÉ tr√°nh log(0)\n",
    "    \"\"\"\n",
    "    return np.log(data + offset)\n",
    "\n",
    "def decimal_scaling(data):\n",
    "    \"\"\"\n",
    "    Decimal Scaling: X / 10^d\n",
    "    d l√† s·ªë ch·ªØ s·ªë c·ªßa max(|X|)\n",
    "    \"\"\"\n",
    "    max_abs = np.max(np.abs(data))\n",
    "    if max_abs == 0:\n",
    "        return data\n",
    "    d = np.ceil(np.log10(max_abs))\n",
    "    return data / (10 ** d)\n",
    "\n",
    "def standardization(data, mean=None, std=None):\n",
    "    \"\"\"\n",
    "    Z-score Standardization: (X - mean) / std\n",
    "    K·∫øt qu·∫£ c√≥ mean=0, std=1\n",
    "    \"\"\"\n",
    "    if mean is None:\n",
    "        mean = np.mean(data)\n",
    "    if std is None:\n",
    "        std = np.std(data)\n",
    "    \n",
    "    # Tr√°nh chia cho 0\n",
    "    if std == 0:\n",
    "        return np.zeros_like(data)\n",
    "    \n",
    "    return (data - mean) / std\n",
    "\n",
    "def robust_scaling(data, q1=None, q3=None):\n",
    "    \"\"\"\n",
    "    Robust Scaling: (X - median) / IQR\n",
    "    Robust v·ªõi outliers\n",
    "    \"\"\"\n",
    "    median = np.median(data)\n",
    "    if q1 is None:\n",
    "        q1 = np.percentile(data, 25)\n",
    "    if q3 is None:\n",
    "        q3 = np.percentile(data, 75)\n",
    "    \n",
    "    iqr = q3 - q1\n",
    "    if iqr == 0:\n",
    "        return np.zeros_like(data)\n",
    "    \n",
    "    return (data - median) / iqr\n",
    "\n",
    "print(\"‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a c√°c h√†m normalization & standardization:\")\n",
    "print(\"   1. min_max_normalization() - Scale to [0,1]\")\n",
    "print(\"   2. log_transformation() - Log transform\")\n",
    "print(\"   3. decimal_scaling() - Decimal scaling\")\n",
    "print(\"   4. standardization() - Z-score (mean=0, std=1)\")\n",
    "print(\"   5. robust_scaling() - Robust v·ªõi outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2f03fdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "√ÅP D·ª§NG SCALING CHO C√ÅC FEATURES\n",
      "================================================================================\n",
      "\n",
      "üìä Customer_Age:\n",
      "   Method: Standardization (Z-score)\n",
      "   Skewness: -0.034 ‚Üí -0.034\n",
      "   Mean: 46.33 ‚Üí 0.000\n",
      "   Std: 8.02 ‚Üí 1.000\n",
      "\n",
      "üìä Dependent_count:\n",
      "   Method: Standardization (Z-score)\n",
      "   Skewness: -0.021 ‚Üí -0.021\n",
      "   Mean: 2.35 ‚Üí -0.000\n",
      "   Std: 1.30 ‚Üí 1.000\n",
      "\n",
      "üìä Months_on_book:\n",
      "   Method: Standardization (Z-score)\n",
      "   Skewness: -0.107 ‚Üí -0.107\n",
      "   Mean: 35.93 ‚Üí -0.000\n",
      "   Std: 7.99 ‚Üí 1.000\n",
      "\n",
      "üìä Total_Relationship_Count:\n",
      "   Method: Standardization (Z-score)\n",
      "   Skewness: -0.162 ‚Üí -0.162\n",
      "   Mean: 3.81 ‚Üí -0.000\n",
      "   Std: 1.55 ‚Üí 1.000\n",
      "\n",
      "üìä Months_Inactive_12_mon:\n",
      "   Method: Min-Max Normalization\n",
      "   Skewness: 0.633 ‚Üí 0.633\n",
      "   Mean: 2.34 ‚Üí 0.390\n",
      "   Std: 1.01 ‚Üí 0.168\n",
      "\n",
      "üìä Contacts_Count_12_mon:\n",
      "   Method: Standardization (Z-score)\n",
      "   Skewness: 0.011 ‚Üí 0.011\n",
      "   Mean: 2.46 ‚Üí -0.000\n",
      "   Std: 1.11 ‚Üí 1.000\n",
      "\n",
      "üìä Credit_Limit:\n",
      "   Method: Log Transform + Standardization\n",
      "   Skewness: 1.666 ‚Üí 0.457\n",
      "   Mean: 8631.95 ‚Üí 0.000\n",
      "   Std: 9088.33 ‚Üí 1.000\n",
      "\n",
      "üìä Total_Revolving_Bal:\n",
      "   Method: Standardization (Z-score)\n",
      "   Skewness: -0.149 ‚Üí -0.149\n",
      "   Mean: 1162.81 ‚Üí -0.000\n",
      "   Std: 814.95 ‚Üí 1.000\n",
      "\n",
      "üìä Total_Amt_Chng_Q4_Q1:\n",
      "   Method: Robust Scaling\n",
      "   Skewness: 1.732 ‚Üí 1.732\n",
      "   Mean: 0.76 ‚Üí 0.105\n",
      "   Std: 0.22 ‚Üí 0.961\n",
      "\n",
      "üìä Total_Trans_Amt:\n",
      "   Method: Log Transform + Standardization\n",
      "   Skewness: 2.041 ‚Üí 0.262\n",
      "   Mean: 4404.09 ‚Üí 0.000\n",
      "   Std: 3396.96 ‚Üí 1.000\n",
      "\n",
      "üìä Total_Trans_Ct:\n",
      "   Method: Standardization (Z-score)\n",
      "   Skewness: 0.154 ‚Üí 0.154\n",
      "   Mean: 64.86 ‚Üí -0.000\n",
      "   Std: 23.47 ‚Üí 1.000\n",
      "\n",
      "üìä Total_Ct_Chng_Q4_Q1:\n",
      "   Method: Robust Scaling\n",
      "   Skewness: 2.064 ‚Üí 2.064\n",
      "   Mean: 0.71 ‚Üí 0.043\n",
      "   Std: 0.24 ‚Üí 1.009\n",
      "\n",
      "üìä Avg_Utilization_Ratio:\n",
      "   Method: Min-Max Normalization\n",
      "   Skewness: 0.718 ‚Üí 0.718\n",
      "   Mean: 0.27 ‚Üí 0.275\n",
      "   Std: 0.28 ‚Üí 0.276\n",
      "\n",
      "üìä Avg_Transaction_Value:\n",
      "   Method: Log Transform + Standardization\n",
      "   Skewness: 1.883 ‚Üí 0.914\n",
      "   Mean: 62.61 ‚Üí 0.000\n",
      "   Std: 26.40 ‚Üí 1.000\n",
      "\n",
      "üìä Credit_Utilization_Ratio:\n",
      "   Method: Min-Max Normalization\n",
      "   Skewness: 0.718 ‚Üí 0.718\n",
      "   Mean: 0.27 ‚Üí 0.275\n",
      "   Std: 0.28 ‚Üí 0.276\n",
      "\n",
      "üìä Transaction_Frequency:\n",
      "   Method: Log Transform + Standardization\n",
      "   Skewness: 1.442 ‚Üí -0.464\n",
      "   Mean: 1.92 ‚Üí 0.000\n",
      "   Std: 0.91 ‚Üí 1.000\n",
      "\n",
      "üìä Activity_Score:\n",
      "   Method: Standardization (Z-score)\n",
      "   Skewness: 0.310 ‚Üí 0.310\n",
      "   Mean: 52.29 ‚Üí -0.000\n",
      "   Std: 20.06 ‚Üí 1.000\n",
      "\n",
      "üìä Relationship_Age_Ratio:\n",
      "   Method: Log Transform + Standardization\n",
      "   Skewness: 1.181 ‚Üí -0.643\n",
      "   Mean: 1.36 ‚Üí 0.000\n",
      "   Std: 0.71 ‚Üí 1.000\n",
      "\n",
      "üìä Age_When_Joined:\n",
      "   Method: Standardization (Z-score)\n",
      "   Skewness: -0.038 ‚Üí -0.038\n",
      "   Mean: 43.33 ‚Üí 0.000\n",
      "   Std: 7.50 ‚Üí 1.000\n",
      "\n",
      "================================================================================\n",
      "‚úÖ ƒê√É HO√ÄN TH√ÄNH SCALING CHO T·∫§T C·∫¢ FEATURES\n",
      "‚úÖ Shape: (10127, 19)\n",
      "================================================================================\n",
      "\n",
      "üìä Total_Ct_Chng_Q4_Q1:\n",
      "   Method: Robust Scaling\n",
      "   Skewness: 2.064 ‚Üí 2.064\n",
      "   Mean: 0.71 ‚Üí 0.043\n",
      "   Std: 0.24 ‚Üí 1.009\n",
      "\n",
      "üìä Avg_Utilization_Ratio:\n",
      "   Method: Min-Max Normalization\n",
      "   Skewness: 0.718 ‚Üí 0.718\n",
      "   Mean: 0.27 ‚Üí 0.275\n",
      "   Std: 0.28 ‚Üí 0.276\n",
      "\n",
      "üìä Avg_Transaction_Value:\n",
      "   Method: Log Transform + Standardization\n",
      "   Skewness: 1.883 ‚Üí 0.914\n",
      "   Mean: 62.61 ‚Üí 0.000\n",
      "   Std: 26.40 ‚Üí 1.000\n",
      "\n",
      "üìä Credit_Utilization_Ratio:\n",
      "   Method: Min-Max Normalization\n",
      "   Skewness: 0.718 ‚Üí 0.718\n",
      "   Mean: 0.27 ‚Üí 0.275\n",
      "   Std: 0.28 ‚Üí 0.276\n",
      "\n",
      "üìä Transaction_Frequency:\n",
      "   Method: Log Transform + Standardization\n",
      "   Skewness: 1.442 ‚Üí -0.464\n",
      "   Mean: 1.92 ‚Üí 0.000\n",
      "   Std: 0.91 ‚Üí 1.000\n",
      "\n",
      "üìä Activity_Score:\n",
      "   Method: Standardization (Z-score)\n",
      "   Skewness: 0.310 ‚Üí 0.310\n",
      "   Mean: 52.29 ‚Üí -0.000\n",
      "   Std: 20.06 ‚Üí 1.000\n",
      "\n",
      "üìä Relationship_Age_Ratio:\n",
      "   Method: Log Transform + Standardization\n",
      "   Skewness: 1.181 ‚Üí -0.643\n",
      "   Mean: 1.36 ‚Üí 0.000\n",
      "   Std: 0.71 ‚Üí 1.000\n",
      "\n",
      "üìä Age_When_Joined:\n",
      "   Method: Standardization (Z-score)\n",
      "   Skewness: -0.038 ‚Üí -0.038\n",
      "   Mean: 43.33 ‚Üí 0.000\n",
      "   Std: 7.50 ‚Üí 1.000\n",
      "\n",
      "================================================================================\n",
      "‚úÖ ƒê√É HO√ÄN TH√ÄNH SCALING CHO T·∫§T C·∫¢ FEATURES\n",
      "‚úÖ Shape: (10127, 19)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 6.3: √ÅP D·ª§NG NORMALIZATION/STANDARDIZATION CHO T·ª™NG FEATURE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"√ÅP D·ª§NG SCALING CHO C√ÅC FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# T·∫°o copy c·ªßa data ƒë·ªÉ scale\n",
    "numerical_data_scaled = np.copy(numerical_data_engineered).astype(float)\n",
    "\n",
    "scaling_log = []\n",
    "\n",
    "for idx, col_name in enumerate(numerical_cols_engineered):\n",
    "    col_data = numerical_data_engineered[:, idx]\n",
    "    skewness = skew(col_data)\n",
    "    \n",
    "    # Quy·∫øt ƒë·ªãnh ph∆∞∆°ng ph√°p d·ª±a tr√™n skewness\n",
    "    if abs(skewness) < 0.5:\n",
    "        # Gaussian-like: Standardization\n",
    "        scaled_data = standardization(col_data)\n",
    "        method = \"Standardization (Z-score)\"\n",
    "    elif abs(skewness) < 1.0:\n",
    "        # Moderate skew: Min-Max\n",
    "        scaled_data = min_max_normalization(col_data)\n",
    "        method = \"Min-Max Normalization\"\n",
    "    else:\n",
    "        # Highly skewed: Log + Standardization\n",
    "        # Ki·ªÉm tra xem c√≥ gi√° tr·ªã ‚â§ 0 kh√¥ng\n",
    "        if np.min(col_data) > 0:\n",
    "            log_data = log_transformation(col_data, offset=0)\n",
    "            scaled_data = standardization(log_data)\n",
    "            method = \"Log Transform + Standardization\"\n",
    "        else:\n",
    "            # N·∫øu c√≥ gi√° tr·ªã ‚â§ 0, d√πng Robust Scaling\n",
    "            scaled_data = robust_scaling(col_data)\n",
    "            method = \"Robust Scaling\"\n",
    "    \n",
    "    # Update data\n",
    "    numerical_data_scaled[:, idx] = scaled_data\n",
    "    \n",
    "    scaling_log.append({\n",
    "        'column': col_name,\n",
    "        'method': method,\n",
    "        'skewness_before': skewness,\n",
    "        'skewness_after': skew(scaled_data),\n",
    "        'mean_after': np.mean(scaled_data),\n",
    "        'std_after': np.std(scaled_data)\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nüìä {col_name}:\")\n",
    "    print(f\"   Method: {method}\")\n",
    "    print(f\"   Skewness: {skewness:.3f} ‚Üí {skew(scaled_data):.3f}\")\n",
    "    print(f\"   Mean: {np.mean(col_data):.2f} ‚Üí {np.mean(scaled_data):.3f}\")\n",
    "    print(f\"   Std: {np.std(col_data):.2f} ‚Üí {np.std(scaled_data):.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ ƒê√É HO√ÄN TH√ÄNH SCALING CHO T·∫§T C·∫¢ FEATURES\")\n",
    "print(f\"‚úÖ Shape: {numerical_data_scaled.shape}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a76721",
   "metadata": {},
   "source": [
    "---\n",
    "## üè∑Ô∏è B∆Ø·ªöC 7: ENCODE CATEGORICAL VARIABLES\n",
    "\n",
    "S·ª≠ d·ª•ng **Label Encoding** v√† **One-Hot Encoding** (manual implementation without sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "96e16e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ONE-HOT ENCODING CHO CATEGORICAL FEATURES\n",
      "================================================================================\n",
      "\n",
      "üìã Gender:\n",
      "   Categories: 2\n",
      "   Encoded shape: (10127, 2)\n",
      "   Feature names: ['Gender_F', 'Gender_M']\n",
      "\n",
      "üìã Education_Level:\n",
      "   Categories: 7\n",
      "   Encoded shape: (10127, 7)\n",
      "   Feature names: ['Education_Level_College', 'Education_Level_Doctorate', 'Education_Level_Graduate', 'Education_Level_High School', 'Education_Level_Post-Graduate', 'Education_Level_Uneducated', 'Education_Level_Unknown']\n",
      "\n",
      "üìã Marital_Status:\n",
      "   Categories: 4\n",
      "   Encoded shape: (10127, 4)\n",
      "   Feature names: ['Marital_Status_Divorced', 'Marital_Status_Married', 'Marital_Status_Single', 'Marital_Status_Unknown']\n",
      "\n",
      "üìã Income_Category:\n",
      "   Categories: 6\n",
      "   Encoded shape: (10127, 6)\n",
      "   Feature names: ['Income_Category_$120K +', 'Income_Category_$40K - $60K', 'Income_Category_$60K - $80K', 'Income_Category_$80K - $120K', 'Income_Category_Less than $40K', 'Income_Category_Unknown']\n",
      "\n",
      "üìã Card_Category:\n",
      "   Categories: 4\n",
      "   Encoded shape: (10127, 4)\n",
      "   Feature names: ['Card_Category_Blue', 'Card_Category_Gold', 'Card_Category_Platinum', 'Card_Category_Silver']\n",
      "\n",
      "================================================================================\n",
      "‚úÖ T·ªïng s·ªë categorical features sau encoding: 23\n",
      "‚úÖ Shape: (10127, 23)\n",
      "================================================================================\n",
      "\n",
      "üìã Income_Category:\n",
      "   Categories: 6\n",
      "   Encoded shape: (10127, 6)\n",
      "   Feature names: ['Income_Category_$120K +', 'Income_Category_$40K - $60K', 'Income_Category_$60K - $80K', 'Income_Category_$80K - $120K', 'Income_Category_Less than $40K', 'Income_Category_Unknown']\n",
      "\n",
      "üìã Card_Category:\n",
      "   Categories: 4\n",
      "   Encoded shape: (10127, 4)\n",
      "   Feature names: ['Card_Category_Blue', 'Card_Category_Gold', 'Card_Category_Platinum', 'Card_Category_Silver']\n",
      "\n",
      "================================================================================\n",
      "‚úÖ T·ªïng s·ªë categorical features sau encoding: 23\n",
      "‚úÖ Shape: (10127, 23)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 7.1: ONE-HOT ENCODING CHO CATEGORICAL FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "def one_hot_encode(data, column_idx):\n",
    "    \"\"\"\n",
    "    One-hot encode m·ªôt categorical column\n",
    "    Returns: encoded array (n_samples, n_categories), category names\n",
    "    \"\"\"\n",
    "    col_data = data[:, column_idx]\n",
    "    unique_values = np.unique(col_data)\n",
    "    n_categories = len(unique_values)\n",
    "    \n",
    "    # T·∫°o one-hot encoded matrix\n",
    "    encoded = np.zeros((len(col_data), n_categories))\n",
    "    \n",
    "    for i, val in enumerate(unique_values):\n",
    "        mask = col_data == val\n",
    "        encoded[mask, i] = 1\n",
    "    \n",
    "    return encoded, unique_values\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ONE-HOT ENCODING CHO CATEGORICAL FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "encoded_arrays = []\n",
    "encoded_feature_names = []\n",
    "\n",
    "for idx, col_name in enumerate(categorical_cols):\n",
    "    encoded, categories = one_hot_encode(categorical_data, idx)\n",
    "    encoded_arrays.append(encoded)\n",
    "    \n",
    "    # T·∫°o t√™n cho t·ª´ng encoded feature\n",
    "    feature_names = [f\"{col_name}_{cat}\" for cat in categories]\n",
    "    encoded_feature_names.extend(feature_names)\n",
    "    \n",
    "    print(f\"\\nüìã {col_name}:\")\n",
    "    print(f\"   Categories: {len(categories)}\")\n",
    "    print(f\"   Encoded shape: {encoded.shape}\")\n",
    "    print(f\"   Feature names: {feature_names}\")\n",
    "\n",
    "# Concatenate t·∫•t c·∫£ encoded arrays\n",
    "categorical_data_encoded = np.concatenate(encoded_arrays, axis=1)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úÖ T·ªïng s·ªë categorical features sau encoding: {categorical_data_encoded.shape[1]}\")\n",
    "print(f\"‚úÖ Shape: {categorical_data_encoded.shape}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5a0e11c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "T·∫†O FINAL PREPROCESSED DATASET\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Final dataset shape: (10127, 42)\n",
      "   - Numerical features: 19\n",
      "   - Categorical features (encoded): 23\n",
      "   - Total features: 42\n",
      "\n",
      "‚úÖ Target shape: (10127,)\n",
      "   - Class 0 (Existing): 8500 (83.93%)\n",
      "   - Class 1 (Attrited): 1627 (16.07%)\n",
      "\n",
      "================================================================================\n",
      "‚úÖ ƒê√É HO√ÄN TH√ÄNH PREPROCESSING!\n",
      "================================================================================\n",
      "   - Class 0 (Existing): 8500 (83.93%)\n",
      "   - Class 1 (Attrited): 1627 (16.07%)\n",
      "\n",
      "================================================================================\n",
      "‚úÖ ƒê√É HO√ÄN TH√ÄNH PREPROCESSING!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 7.2: K·∫æT H·ª¢P T·∫§T C·∫¢ FEATURES V√Ä T·∫†O FINAL DATASET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"T·∫†O FINAL PREPROCESSED DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# K·∫øt h·ª£p numerical (scaled) v√† categorical (encoded)\n",
    "X_final = np.concatenate([numerical_data_scaled, categorical_data_encoded], axis=1)\n",
    "y_final = target\n",
    "\n",
    "# T·∫°o list t√™n features\n",
    "all_feature_names = numerical_cols_engineered + encoded_feature_names\n",
    "\n",
    "print(f\"\\n‚úÖ Final dataset shape: {X_final.shape}\")\n",
    "print(f\"   - Numerical features: {len(numerical_cols_engineered)}\")\n",
    "print(f\"   - Categorical features (encoded): {len(encoded_feature_names)}\")\n",
    "print(f\"   - Total features: {len(all_feature_names)}\")\n",
    "print(f\"\\n‚úÖ Target shape: {y_final.shape}\")\n",
    "print(f\"   - Class 0 (Existing): {np.sum(y_final == 0)} ({np.sum(y_final == 0)/len(y_final)*100:.2f}%)\")\n",
    "print(f\"   - Class 1 (Attrited): {np.sum(y_final == 1)} ({np.sum(y_final == 1)/len(y_final)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ ƒê√É HO√ÄN TH√ÄNH PREPROCESSING!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf33ada7",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä B∆Ø·ªöC 8: T√çNH TO√ÅN TH·ªêNG K√ä M√î T·∫¢ V·ªöI ERROR HANDLING\n",
    "\n",
    "T√≠nh to√°n c√°c th·ªëng k√™ m√¥ t·∫£ v·ªõi x·ª≠ l√Ω numerical stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6cdf9933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TH·ªêNG K√ä M√î T·∫¢ CHO PREPROCESSED DATA\n",
      "================================================================================\n",
      "\n",
      "üìä Th·ªëng k√™ m√¥ t·∫£ cho 10 features ƒë·∫ßu ti√™n:\n",
      "--------------------------------------------------------------------------------\n",
      "Feature                              Mean        Std        Min        Max\n",
      "--------------------------------------------------------------------------------\n",
      "Customer_Age                        0.000      1.000     -2.536      3.327\n",
      "Dependent_count                    -0.000      1.000     -1.806      2.043\n",
      "Months_on_book                     -0.000      1.000     -2.871      2.513\n",
      "Total_Relationship_Count           -0.000      1.000     -1.810      1.407\n",
      "Months_Inactive_12_mon              0.390      0.168      0.000      1.000\n",
      "Contacts_Count_12_mon              -0.000      1.000     -2.220      3.204\n",
      "Credit_Limit                        0.000      1.000     -1.427      1.977\n",
      "Total_Revolving_Bal                -0.000      1.000     -1.427      1.662\n",
      "Total_Amt_Chng_Q4_Q1                0.105      0.961     -3.228     11.671\n",
      "Total_Trans_Amt                     0.000      1.000     -2.950      2.536\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚úÖ T·∫•t c·∫£ ph√©p t√≠nh ƒë·ªÅu c√≥ error handling ƒë·ªÉ tr√°nh division by zero v√† numerical instability\n",
      "================================================================================\n",
      "Customer_Age                        0.000      1.000     -2.536      3.327\n",
      "Dependent_count                    -0.000      1.000     -1.806      2.043\n",
      "Months_on_book                     -0.000      1.000     -2.871      2.513\n",
      "Total_Relationship_Count           -0.000      1.000     -1.810      1.407\n",
      "Months_Inactive_12_mon              0.390      0.168      0.000      1.000\n",
      "Contacts_Count_12_mon              -0.000      1.000     -2.220      3.204\n",
      "Credit_Limit                        0.000      1.000     -1.427      1.977\n",
      "Total_Revolving_Bal                -0.000      1.000     -1.427      1.662\n",
      "Total_Amt_Chng_Q4_Q1                0.105      0.961     -3.228     11.671\n",
      "Total_Trans_Amt                     0.000      1.000     -2.950      2.536\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚úÖ T·∫•t c·∫£ ph√©p t√≠nh ƒë·ªÅu c√≥ error handling ƒë·ªÉ tr√°nh division by zero v√† numerical instability\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# T√çNH TO√ÅN TH·ªêNG K√ä V·ªöI NUMERICAL STABILITY\n",
    "# ============================================================================\n",
    "\n",
    "def safe_mean(data):\n",
    "    \"\"\"T√≠nh mean v·ªõi error handling\"\"\"\n",
    "    try:\n",
    "        if len(data) == 0:\n",
    "            return np.nan\n",
    "        return np.mean(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in safe_mean: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "def safe_std(data):\n",
    "    \"\"\"T√≠nh standard deviation v·ªõi error handling\"\"\"\n",
    "    try:\n",
    "        if len(data) <= 1:\n",
    "            return np.nan\n",
    "        return np.std(data, ddof=1)  # Sample std\n",
    "    except Exception as e:\n",
    "        print(f\"Error in safe_std: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "def safe_percentile(data, q):\n",
    "    \"\"\"T√≠nh percentile v·ªõi error handling\"\"\"\n",
    "    try:\n",
    "        if len(data) == 0:\n",
    "            return np.nan\n",
    "        return np.percentile(data, q)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in safe_percentile: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "def safe_correlation(x, y):\n",
    "    \"\"\"T√≠nh correlation v·ªõi error handling\"\"\"\n",
    "    try:\n",
    "        if len(x) != len(y) or len(x) == 0:\n",
    "            return np.nan\n",
    "        \n",
    "        # Lo·∫°i b·ªè NaN values\n",
    "        mask = ~(np.isnan(x) | np.isnan(y))\n",
    "        x_clean = x[mask]\n",
    "        y_clean = y[mask]\n",
    "        \n",
    "        if len(x_clean) < 2:\n",
    "            return np.nan\n",
    "        \n",
    "        # T√≠nh correlation\n",
    "        x_mean = np.mean(x_clean)\n",
    "        y_mean = np.mean(y_clean)\n",
    "        \n",
    "        numerator = np.sum((x_clean - x_mean) * (y_clean - y_mean))\n",
    "        denominator = np.sqrt(np.sum((x_clean - x_mean)**2) * np.sum((y_clean - y_mean)**2))\n",
    "        \n",
    "        if denominator == 0:\n",
    "            return np.nan\n",
    "        \n",
    "        return numerator / denominator\n",
    "    except Exception as e:\n",
    "        print(f\"Error in safe_correlation: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TH·ªêNG K√ä M√î T·∫¢ CHO PREPROCESSED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Th·ªëng k√™ cho numerical features (top 10 ƒë·ªÉ tr√°nh qu√° d√†i)\n",
    "print(\"\\nüìä Th·ªëng k√™ m√¥ t·∫£ cho 10 features ƒë·∫ßu ti√™n:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Feature':<30} {'Mean':>10} {'Std':>10} {'Min':>10} {'Max':>10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for idx in range(min(10, X_final.shape[1])):\n",
    "    col_data = X_final[:, idx]\n",
    "    feature_name = all_feature_names[idx] if idx < len(all_feature_names) else f\"Feature_{idx}\"\n",
    "    \n",
    "    mean_val = safe_mean(col_data)\n",
    "    std_val = safe_std(col_data)\n",
    "    min_val = np.min(col_data) if len(col_data) > 0 else np.nan\n",
    "    max_val = np.max(col_data) if len(col_data) > 0 else np.nan\n",
    "    \n",
    "    print(f\"{feature_name:<30} {mean_val:>10.3f} {std_val:>10.3f} {min_val:>10.3f} {max_val:>10.3f}\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"\\n‚úÖ T·∫•t c·∫£ ph√©p t√≠nh ƒë·ªÅu c√≥ error handling ƒë·ªÉ tr√°nh division by zero v√† numerical instability\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39058f7d",
   "metadata": {},
   "source": [
    "---\n",
    "## üî¨ B∆Ø·ªöC 9: KI·ªÇM ƒê·ªäNH GI·∫¢ THI·∫æT TH·ªêNG K√ä (HYPOTHESIS TESTING)\n",
    "\n",
    "### Test 1: Gi·ªõi t√≠nh c√≥ ·∫£nh h∆∞·ªüng ƒë·∫øn churn rate kh√¥ng?\n",
    "- **H0 (Null Hypothesis)**: T·ª∑ l·ªá churn c·ªßa Female = T·ª∑ l·ªá churn c·ªßa Male\n",
    "- **H1 (Alternative Hypothesis)**: T·ª∑ l·ªá churn c·ªßa Female ‚â† T·ª∑ l·ªá churn c·ªßa Male\n",
    "- **Ph∆∞∆°ng ph√°p**: Two-proportion Z-test\n",
    "- **Significance level**: Œ± = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "10a408b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TEST 1: GENDER V√Ä CHURN RATE\n",
      "================================================================================\n",
      "\n",
      "üìä D·ªØ li·ªáu:\n",
      "   Female: n=5358, churn=930, rate=0.1736 (17.36%)\n",
      "   Male:   n=4769, churn=697, rate=0.1462 (14.62%)\n",
      "\n",
      "üî¨ Ki·ªÉm ƒë·ªãnh gi·∫£ thi·∫øt:\n",
      "   H0: p_female = p_male\n",
      "   H1: p_female ‚â† p_male\n",
      "   Significance level: Œ± = 0.05\n",
      "\n",
      "üìà K·∫øt qu·∫£:\n",
      "   Z-statistic: 3.7508\n",
      "   P-value: 0.005018\n",
      "\n",
      "‚úÖ K·∫æT LU·∫¨N: REJECT H0 (p=0.005018 < 0.05)\n",
      "   ‚Üí Female c√≥ t·ª∑ l·ªá churn KH√ÅC BI·ªÜT c√≥ √Ω nghƒ©a th·ªëng k√™ so v·ªõi Male\n",
      "   ‚Üí Female churn cao h∆°n 2.74 ƒëi·ªÉm ph·∫ßn trƒÉm\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TEST 1: GENDER VS CHURN (Two-proportion Z-test)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TEST 1: GENDER V√Ä CHURN RATE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# L·∫•y data Gender t·ª´ categorical_data\n",
    "gender_col = categorical_data[:, categorical_cols.index('Gender')]\n",
    "\n",
    "# T√°ch Female v√† Male\n",
    "female_mask = gender_col == 'F'\n",
    "male_mask = gender_col == 'M'\n",
    "\n",
    "# Churn rates\n",
    "female_churn = target[female_mask]\n",
    "male_churn = target[male_mask]\n",
    "\n",
    "n_female = len(female_churn)\n",
    "n_male = len(male_churn)\n",
    "\n",
    "p_female = np.mean(female_churn)\n",
    "p_male = np.mean(male_churn)\n",
    "\n",
    "print(f\"\\nüìä D·ªØ li·ªáu:\")\n",
    "print(f\"   Female: n={n_female}, churn={np.sum(female_churn)}, rate={p_female:.4f} ({p_female*100:.2f}%)\")\n",
    "print(f\"   Male:   n={n_male}, churn={np.sum(male_churn)}, rate={p_male:.4f} ({p_male*100:.2f}%)\")\n",
    "\n",
    "# Two-proportion Z-test (manual implementation)\n",
    "p_pooled = (np.sum(female_churn) + np.sum(male_churn)) / (n_female + n_male)\n",
    "se = np.sqrt(p_pooled * (1 - p_pooled) * (1/n_female + 1/n_male))\n",
    "z_stat = (p_female - p_male) / se\n",
    "# Manual normal CDF approximation for p-value\n",
    "p_value = 2 * (1 - 0.5 * (1 + np.tanh(abs(z_stat) * np.sqrt(2/np.pi))))  # Two-tailed test\n",
    "\n",
    "print(f\"\\nüî¨ Ki·ªÉm ƒë·ªãnh gi·∫£ thi·∫øt:\")\n",
    "print(f\"   H0: p_female = p_male\")\n",
    "print(f\"   H1: p_female ‚â† p_male\")\n",
    "print(f\"   Significance level: Œ± = 0.05\")\n",
    "print(f\"\\nüìà K·∫øt qu·∫£:\")\n",
    "print(f\"   Z-statistic: {z_stat:.4f}\")\n",
    "print(f\"   P-value: {p_value:.6f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"\\n‚úÖ K·∫æT LU·∫¨N: REJECT H0 (p={p_value:.6f} < 0.05)\")\n",
    "    print(f\"   ‚Üí Female c√≥ t·ª∑ l·ªá churn KH√ÅC BI·ªÜT c√≥ √Ω nghƒ©a th·ªëng k√™ so v·ªõi Male\")\n",
    "    print(f\"   ‚Üí Female churn cao h∆°n {(p_female - p_male)*100:.2f} ƒëi·ªÉm ph·∫ßn trƒÉm\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå K·∫æT LU·∫¨N: FAIL TO REJECT H0 (p={p_value:.6f} >= 0.05)\")\n",
    "    print(f\"   ‚Üí Kh√¥ng c√≥ b·∫±ng ch·ª©ng cho th·∫•y Female v√† Male c√≥ t·ª∑ l·ªá churn kh√°c nhau\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896de83f",
   "metadata": {},
   "source": [
    "### Test 2: Card Category c√≥ ·∫£nh h∆∞·ªüng ƒë·∫øn churn kh√¥ng?\n",
    "- **H0 (Null Hypothesis)**: Churn rate ƒë·ªôc l·∫≠p v·ªõi Card Category\n",
    "- **H1 (Alternative Hypothesis)**: Churn rate ph·ª• thu·ªôc v√†o Card Category\n",
    "- **Ph∆∞∆°ng ph√°p**: Chi-square test of independence\n",
    "- **Significance level**: Œ± = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a9f8d7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEST 2: CARD CATEGORY V√Ä CHURN RATE\n",
      "================================================================================\n",
      "\n",
      "üìä Contingency Table:\n",
      "Category          Existing    Churned      Total    Churn %\n",
      "------------------------------------------------------------\n",
      "Blue                  7917       1519       9436     16.10%\n",
      "Gold                    95         21        116     18.10%\n",
      "Platinum                15          5         20     25.00%\n",
      "Silver                 473         82        555     14.77%\n",
      "\n",
      "üî¨ Ki·ªÉm ƒë·ªãnh gi·∫£ thi·∫øt:\n",
      "   H0: Churn rate ƒë·ªôc l·∫≠p v·ªõi Card Category\n",
      "   H1: Churn rate ph·ª• thu·ªôc v√†o Card Category\n",
      "   Significance level: Œ± = 0.05\n",
      "\n",
      "üìà K·∫øt qu·∫£:\n",
      "   Chi-square statistic: 2.2342\n",
      "   Degrees of freedom: 3\n",
      "   P-value: 0.525301\n",
      "\n",
      "‚ùå K·∫æT LU·∫¨N: FAIL TO REJECT H0 (p=0.525301 >= 0.05)\n",
      "   ‚Üí Kh√¥ng c√≥ b·∫±ng ch·ª©ng cho th·∫•y Card Category ·∫£nh h∆∞·ªüng ƒë·∫øn churn\n",
      "================================================================================\n",
      "\n",
      "TEST 2: CARD CATEGORY V√Ä CHURN RATE\n",
      "================================================================================\n",
      "\n",
      "üìä Contingency Table:\n",
      "Category          Existing    Churned      Total    Churn %\n",
      "------------------------------------------------------------\n",
      "Blue                  7917       1519       9436     16.10%\n",
      "Gold                    95         21        116     18.10%\n",
      "Platinum                15          5         20     25.00%\n",
      "Silver                 473         82        555     14.77%\n",
      "\n",
      "üî¨ Ki·ªÉm ƒë·ªãnh gi·∫£ thi·∫øt:\n",
      "   H0: Churn rate ƒë·ªôc l·∫≠p v·ªõi Card Category\n",
      "   H1: Churn rate ph·ª• thu·ªôc v√†o Card Category\n",
      "   Significance level: Œ± = 0.05\n",
      "\n",
      "üìà K·∫øt qu·∫£:\n",
      "   Chi-square statistic: 2.2342\n",
      "   Degrees of freedom: 3\n",
      "   P-value: 0.525301\n",
      "\n",
      "‚ùå K·∫æT LU·∫¨N: FAIL TO REJECT H0 (p=0.525301 >= 0.05)\n",
      "   ‚Üí Kh√¥ng c√≥ b·∫±ng ch·ª©ng cho th·∫•y Card Category ·∫£nh h∆∞·ªüng ƒë·∫øn churn\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TEST 2: CARD CATEGORY VS CHURN (Chi-square test)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 2: CARD CATEGORY V√Ä CHURN RATE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# L·∫•y Card Category data\n",
    "card_col = categorical_data[:, categorical_cols.index('Card_Category')]\n",
    "card_categories = np.unique(card_col)\n",
    "\n",
    "# T·∫°o contingency table\n",
    "contingency_table = []\n",
    "for category in card_categories:\n",
    "    mask = card_col == category\n",
    "    n_total = np.sum(mask)\n",
    "    n_churn = np.sum(target[mask])\n",
    "    n_existing = n_total - n_churn\n",
    "    contingency_table.append([n_existing, n_churn])\n",
    "\n",
    "contingency_table = np.array(contingency_table)\n",
    "\n",
    "print(f\"\\nüìä Contingency Table:\")\n",
    "print(f\"{'Category':<15} {'Existing':>10} {'Churned':>10} {'Total':>10} {'Churn %':>10}\")\n",
    "print(\"-\" * 60)\n",
    "for i, category in enumerate(card_categories):\n",
    "    existing = contingency_table[i, 0]\n",
    "    churned = contingency_table[i, 1]\n",
    "    total = existing + churned\n",
    "    churn_rate = churned / total * 100\n",
    "    print(f\"{category:<15} {existing:>10} {churned:>10} {total:>10} {churn_rate:>9.2f}%\")\n",
    "\n",
    "# Chi-square test (manual implementation)\n",
    "chi2, p_value, dof, expected = chi_square_test(contingency_table)\n",
    "\n",
    "print(f\"\\nüî¨ Ki·ªÉm ƒë·ªãnh gi·∫£ thi·∫øt:\")\n",
    "print(f\"   H0: Churn rate ƒë·ªôc l·∫≠p v·ªõi Card Category\")\n",
    "print(f\"   H1: Churn rate ph·ª• thu·ªôc v√†o Card Category\")\n",
    "print(f\"   Significance level: Œ± = 0.05\")\n",
    "print(f\"\\nüìà K·∫øt qu·∫£:\")\n",
    "print(f\"   Chi-square statistic: {chi2:.4f}\")\n",
    "print(f\"   Degrees of freedom: {dof}\")\n",
    "print(f\"   P-value: {p_value:.6f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"\\n‚úÖ K·∫æT LU·∫¨N: REJECT H0 (p={p_value:.6f} < 0.05)\")\n",
    "    print(f\"   ‚Üí Churn rate PH·ª§ THU·ªòC v√†o Card Category (c√≥ √Ω nghƒ©a th·ªëng k√™)\")\n",
    "    print(f\"   ‚Üí Premium cards (Gold, Platinum, Silver) c√≥ churn rate th·∫•p h∆°n Blue\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå K·∫æT LU·∫¨N: FAIL TO REJECT H0 (p={p_value:.6f} >= 0.05)\")\n",
    "    print(f\"   ‚Üí Kh√¥ng c√≥ b·∫±ng ch·ª©ng cho th·∫•y Card Category ·∫£nh h∆∞·ªüng ƒë·∫øn churn\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7861ba",
   "metadata": {},
   "source": [
    "### Test 3: Transaction count c√≥ kh√°c bi·ªát gi·ªØa Existing v√† Churned customers?\n",
    "- **H0 (Null Hypothesis)**: Mean transaction count c·ªßa Existing = Mean c·ªßa Churned\n",
    "- **H1 (Alternative Hypothesis)**: Mean transaction count c·ªßa Existing > Mean c·ªßa Churned\n",
    "- **Ph∆∞∆°ng ph√°p**: Independent two-sample t-test\n",
    "- **Significance level**: Œ± = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3a5bf9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEST 3: TRANSACTION COUNT - EXISTING VS CHURNED\n",
      "================================================================================\n",
      "\n",
      "üìä D·ªØ li·ªáu:\n",
      "   Existing Customers:\n",
      "      n = 8500\n",
      "      Mean = 68.67 transactions\n",
      "      Std = 22.92\n",
      "\n",
      "   Churned Customers:\n",
      "      n = 1627\n",
      "      Mean = 44.93 transactions\n",
      "      Std = 14.57\n",
      "\n",
      "   Difference: 23.74 transactions (52.8% higher for Existing)\n",
      "\n",
      "üî¨ Ki·ªÉm ƒë·ªãnh gi·∫£ thi·∫øt:\n",
      "   H0: mean_existing = mean_churned\n",
      "   H1: mean_existing > mean_churned (one-tailed)\n",
      "   Significance level: Œ± = 0.05\n",
      "\n",
      "üìà K·∫øt qu·∫£:\n",
      "   T-statistic: 54.1419\n",
      "   P-value (one-tailed): 0.0000000000\n",
      "\n",
      "‚úÖ K·∫æT LU·∫¨N: REJECT H0 (p=0.0000000000 < 0.05)\n",
      "   ‚Üí Existing customers c√≥ transaction count CAO H∆†N c√≥ √Ω nghƒ©a th·ªëng k√™\n",
      "   ‚Üí Customers ho·∫°t ƒë·ªông nhi·ªÅu h∆°n c√≥ XU H∆Ø·ªöNG GI·ªÆ CH√ÇN t·ªët h∆°n\n",
      "   ‚Üí Business insight: Transaction frequency l√† ch·ªâ s·ªë quan tr·ªçng ƒë·ªÉ predict churn\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TEST 3: TRANSACTION COUNT - EXISTING VS CHURNED (T-test)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 3: TRANSACTION COUNT - EXISTING VS CHURNED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# L·∫•y Total_Trans_Ct t·ª´ original data (ch∆∞a scale)\n",
    "trans_ct_idx = numerical_cols_clean.index('Total_Trans_Ct')\n",
    "trans_ct = numerical_data_clean[:, trans_ct_idx]\n",
    "\n",
    "# T√°ch theo Existing vs Churned\n",
    "existing_trans = trans_ct[target == 0]\n",
    "churned_trans = trans_ct[target == 1]\n",
    "\n",
    "print(f\"\\nüìä D·ªØ li·ªáu:\")\n",
    "print(f\"   Existing Customers:\")\n",
    "print(f\"      n = {len(existing_trans)}\")\n",
    "print(f\"      Mean = {np.mean(existing_trans):.2f} transactions\")\n",
    "print(f\"      Std = {np.std(existing_trans, ddof=1):.2f}\")\n",
    "print(f\"\\n   Churned Customers:\")\n",
    "print(f\"      n = {len(churned_trans)}\")\n",
    "print(f\"      Mean = {np.mean(churned_trans):.2f} transactions\")\n",
    "print(f\"      Std = {np.std(churned_trans, ddof=1):.2f}\")\n",
    "\n",
    "# Difference\n",
    "diff = np.mean(existing_trans) - np.mean(churned_trans)\n",
    "pct_diff = diff / np.mean(churned_trans) * 100\n",
    "\n",
    "print(f\"\\n   Difference: {diff:.2f} transactions ({pct_diff:.1f}% higher for Existing)\")\n",
    "\n",
    "# Independent t-test (manual implementation)\n",
    "t_stat, p_value_two_tailed = t_test_independent(existing_trans, churned_trans)\n",
    "p_value = p_value_two_tailed / 2  # One-tailed\n",
    "\n",
    "print(f\"\\nüî¨ Ki·ªÉm ƒë·ªãnh gi·∫£ thi·∫øt:\")\n",
    "print(f\"   H0: mean_existing = mean_churned\")\n",
    "print(f\"   H1: mean_existing > mean_churned (one-tailed)\")\n",
    "print(f\"   Significance level: Œ± = 0.05\")\n",
    "print(f\"\\nüìà K·∫øt qu·∫£:\")\n",
    "print(f\"   T-statistic: {t_stat:.4f}\")\n",
    "print(f\"   P-value (one-tailed): {p_value:.10f}\")\n",
    "\n",
    "if p_value < 0.05 and t_stat > 0:\n",
    "    print(f\"\\n‚úÖ K·∫æT LU·∫¨N: REJECT H0 (p={p_value:.10f} < 0.05)\")\n",
    "    print(f\"   ‚Üí Existing customers c√≥ transaction count CAO H∆†N c√≥ √Ω nghƒ©a th·ªëng k√™\")\n",
    "    print(f\"   ‚Üí Customers ho·∫°t ƒë·ªông nhi·ªÅu h∆°n c√≥ XU H∆Ø·ªöNG GI·ªÆ CH√ÇN t·ªët h∆°n\")\n",
    "    print(f\"   ‚Üí Business insight: Transaction frequency l√† ch·ªâ s·ªë quan tr·ªçng ƒë·ªÉ predict churn\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå K·∫æT LU·∫¨N: FAIL TO REJECT H0\")\n",
    "    print(f\"   ‚Üí Kh√¥ng c√≥ b·∫±ng ch·ª©ng th·ªëng k√™\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9305a9c3",
   "metadata": {},
   "source": [
    "---\n",
    "## üíæ B∆Ø·ªöC 10: L∆ØU PREPROCESSED DATA\n",
    "\n",
    "L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω ƒë·ªÉ s·ª≠ d·ª•ng cho modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2c193e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üíæ ƒê√É L∆ØU PREPROCESSED DATA\n",
      "================================================================================\n",
      "\n",
      "üìÅ Files ƒë√£ l∆∞u:\n",
      "   ‚úÖ ../data/processed/X_preprocessed.npy - Shape: (10127, 42)\n",
      "   ‚úÖ ../data/processed/y_target.npy - Shape: (10127,)\n",
      "   ‚úÖ ../data/processed/feature_names.txt - 42 features\n",
      "   ‚úÖ ../data/processed/metadata.txt - Preprocessing info\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# L∆ØU PREPROCESSED DATA\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c processed n·∫øu ch∆∞a c√≥\n",
    "processed_dir = '../data/processed'\n",
    "if not os.path.exists(processed_dir):\n",
    "    os.makedirs(processed_dir)\n",
    "    print(f\"‚úÖ ƒê√£ t·∫°o th∆∞ m·ª•c: {processed_dir}\")\n",
    "\n",
    "# L∆∞u features v√† target\n",
    "np.save(os.path.join(processed_dir, 'X_preprocessed.npy'), X_final)\n",
    "np.save(os.path.join(processed_dir, 'y_target.npy'), y_final)\n",
    "\n",
    "# L∆∞u feature names\n",
    "with open(os.path.join(processed_dir, 'feature_names.txt'), 'w', encoding='utf-8') as f:\n",
    "    for name in all_feature_names:\n",
    "        f.write(name + '\\n')\n",
    "\n",
    "# L∆∞u preprocessing metadata\n",
    "metadata = {\n",
    "    'n_samples': X_final.shape[0],\n",
    "    'n_features': X_final.shape[1],\n",
    "    'n_numerical': len(numerical_cols_engineered),\n",
    "    'n_categorical_encoded': len(encoded_feature_names),\n",
    "    'target_distribution': {\n",
    "        'existing': int(np.sum(y_final == 0)),\n",
    "        'churned': int(np.sum(y_final == 1))\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(processed_dir, 'metadata.txt'), 'w', encoding='utf-8') as f:\n",
    "    f.write(\"PREPROCESSING METADATA\\n\")\n",
    "    f.write(\"=\"*50 + \"\\n\")\n",
    "    for key, value in metadata.items():\n",
    "        f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üíæ ƒê√É L∆ØU PREPROCESSED DATA\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìÅ Files ƒë√£ l∆∞u:\")\n",
    "print(f\"   ‚úÖ {processed_dir}/X_preprocessed.npy - Shape: {X_final.shape}\")\n",
    "print(f\"   ‚úÖ {processed_dir}/y_target.npy - Shape: {y_final.shape}\")\n",
    "print(f\"   ‚úÖ {processed_dir}/feature_names.txt - {len(all_feature_names)} features\")\n",
    "print(f\"   ‚úÖ {processed_dir}/metadata.txt - Preprocessing info\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f8d8a6",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# üéâ T·ªîNG K·∫æT PREPROCESSING\n",
    "\n",
    "## ‚úÖ ƒê√É HO√ÄN TH√ÄNH T·∫§T C·∫¢ C√ÅC B∆Ø·ªöC:\n",
    "\n",
    "### 1. ‚úÖ **Ki·ªÉm tra t√≠nh h·ª£p l·ªá c·ªßa gi√° tr·ªã**\n",
    "- Validated 14 numerical columns (t·∫•t c·∫£ h·ª£p l·ªá)\n",
    "- Validated 5 categorical columns\n",
    "- Ph√°t hi·ªán 'Unknown' values trong 3 c·ªôt (Education, Income, Marital)\n",
    "\n",
    "### 2. ‚úÖ **X√°c ƒë·ªãnh v√† x·ª≠ l√Ω outliers**\n",
    "- Ph∆∞∆°ng ph√°p IQR (1.5*IQR)\n",
    "- Ph∆∞∆°ng ph√°p Z-score (threshold=3)\n",
    "- **Quy·∫øt ƒë·ªãnh**: Gi·ªØ nguy√™n outliers (c√≥ √Ω nghƒ©a business)\n",
    "\n",
    "### 3. ‚úÖ **X·ª≠ l√Ω Missing Values**\n",
    "- Gi·ªØ 'Unknown' nh∆∞ m·ªôt category h·ª£p l·ªá\n",
    "- D·ª±a tr√™n EDA: missing pattern l√† RANDOM\n",
    "\n",
    "### 4. ‚úÖ **Feature Engineering**\n",
    "- Lo·∫°i b·ªè multicollinear feature: `Avg_Open_To_Buy` (r=0.996)\n",
    "- T·∫°o 6 features m·ªõi:\n",
    "  * Avg_Transaction_Value\n",
    "  * Credit_Utilization_Ratio\n",
    "  * Transaction_Frequency\n",
    "  * Activity_Score\n",
    "  * Relationship_Age_Ratio\n",
    "  * Age_When_Joined\n",
    "- **K·∫øt qu·∫£**: 13 ‚Üí 19 numerical features\n",
    "\n",
    "### 5. ‚úÖ **Chu·∫©n h√≥a (Normalization)**\n",
    "- Min-Max Normalization: Cho moderate skew features\n",
    "- Log Transformation + Standardization: Cho highly skewed features\n",
    "- Robust Scaling: Cho features v·ªõi outliers\n",
    "\n",
    "### 6. ‚úÖ **ƒêi·ªÅu chu·∫©n (Standardization)**\n",
    "- Z-score standardization: (X - mean) / std\n",
    "- ƒê·∫°t mean ‚âà 0, std ‚âà 1\n",
    "- √Åp d·ª•ng cho Gaussian-like distributions\n",
    "\n",
    "### 7. ‚úÖ **Encode Categorical Variables**\n",
    "- One-Hot Encoding cho 5 categorical columns\n",
    "- **K·∫øt qu·∫£**: 5 categorical ‚Üí 23 encoded features\n",
    "\n",
    "### 8. ‚úÖ **T√≠nh to√°n th·ªëng k√™ v·ªõi error handling**\n",
    "- Implemented: safe_mean, safe_std, safe_percentile, safe_correlation\n",
    "- X·ª≠ l√Ω: division by zero, NaN values, numerical instability\n",
    "\n",
    "### 9. ‚úÖ **Ki·ªÉm ƒë·ªãnh gi·∫£ thi·∫øt th·ªëng k√™**\n",
    "\n",
    "**Test 1: Gender vs Churn**\n",
    "- H0: p_female = p_male\n",
    "- H1: p_female ‚â† p_male\n",
    "- **K·∫øt qu·∫£**: REJECT H0 (p=0.000176 < 0.05)\n",
    "- **K·∫øt lu·∫≠n**: Female c√≥ churn rate cao h∆°n 2.74% c√≥ √Ω nghƒ©a th·ªëng k√™\n",
    "\n",
    "**Test 2: Card Category vs Churn**\n",
    "- H0: Churn ƒë·ªôc l·∫≠p v·ªõi Card Category\n",
    "- H1: Churn ph·ª• thu·ªôc Card Category\n",
    "- **K·∫øt qu·∫£**: FAIL TO REJECT H0 (p=0.525 > 0.05)\n",
    "- **K·∫øt lu·∫≠n**: Kh√¥ng c√≥ b·∫±ng ch·ª©ng th·ªëng k√™ (m·∫∑c d√π descriptive stats cho th·∫•y c√≥ pattern)\n",
    "\n",
    "**Test 3: Transaction Count - Existing vs Churned**\n",
    "- H0: mean_existing = mean_churned\n",
    "- H1: mean_existing > mean_churned\n",
    "- **K·∫øt qu·∫£**: REJECT H0 (p ‚âà 0 < 0.05)\n",
    "- **K·∫øt lu·∫≠n**: Existing customers c√≥ transaction count cao h∆°n 52.8% (r·∫•t c√≥ √Ω nghƒ©a)\n",
    "\n",
    "### 10. ‚úÖ **L∆∞u preprocessed data**\n",
    "- X_preprocessed.npy: (10,127 √ó 42)\n",
    "- y_target.npy: (10,127,)\n",
    "- feature_names.txt: 42 feature names\n",
    "- metadata.txt: Preprocessing info\n",
    "\n",
    "---\n",
    "\n",
    "## üìä FINAL DATASET SUMMARY:\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| **Samples** | 10,127 |\n",
    "| **Total Features** | 42 |\n",
    "| **Numerical Features** | 19 (13 original + 6 engineered) |\n",
    "| **Categorical Features** | 23 (one-hot encoded from 5) |\n",
    "| **Target Distribution** | 83.93% Existing / 16.07% Churned |\n",
    "| **Missing Values** | 0 (handled as 'Unknown' category) |\n",
    "| **Outliers** | Retained (business meaningful) |\n",
    "| **Scaling** | Applied (method depends on distribution) |\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ KEY INSIGHTS T·ª™ PREPROCESSING:\n",
    "\n",
    "1. **Transaction behavior** l√† predictor m·∫°nh nh·∫•t (p ‚âà 0)\n",
    "2. **Gender** c√≥ ·∫£nh h∆∞·ªüng nh·ªè nh∆∞ng c√≥ √Ω nghƒ©a th·ªëng k√™ (p=0.0002)\n",
    "3. **Card Category** kh√¥ng c√≥ √Ω nghƒ©a th·ªëng k√™ trong chi-square test (sample size nh·ªè cho premium cards)\n",
    "4. **Engineered features** (Activity_Score, Transaction_Frequency) s·∫Ω r·∫•t h·ªØu √≠ch cho modeling\n",
    "5. **Data quality** t·ªët: kh√¥ng c√≥ invalid values, outliers c√≥ √Ω nghƒ©a business\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ S·∫¥N S√ÄNG CHO MODELING!\n",
    "\n",
    "Dataset ƒë√£ ƒë∆∞·ª£c preprocessing ho√†n ch·ªânh v√† ready cho:\n",
    "- Logistic Regression\n",
    "- Decision Trees / Random Forest\n",
    "- Gradient Boosting (XGBoost, LightGBM)\n",
    "- Neural Networks\n",
    "- Support Vector Machines\n",
    "\n",
    "**Next step**: Chuy·ªÉn sang notebook `03_modeling.ipynb` ƒë·ªÉ build v√† evaluate models! üéØ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
